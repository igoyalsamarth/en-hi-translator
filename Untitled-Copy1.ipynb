{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d9e9daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import time\n",
    "import torchtext\n",
    "from torch import Tensor\n",
    "from datasets import load_dataset\n",
    "import io\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import spacy\n",
    "from spacy.lang.hi.examples import sentences\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "from torch.nn import (TransformerEncoder, TransformerDecoder, TransformerEncoderLayer, TransformerDecoderLayer)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01a5c7e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Hindi_English_Truncated_Corpus.csv', delimiter = ',')\n",
    "source = df['english_sentence'].tolist()\n",
    "target = df['hindi_sentence'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "020b168a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_train_file = source[0:80]\n",
    "target_train_file = target[0:80]\n",
    "source_valid_file = source[7001:7005]\n",
    "target_valid_file = target[7001:7005]\n",
    "source_test_file = source[3101:3102]\n",
    "target_test_file = target[3101:3102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6fa2833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['politicians do not have permission to do what needs to be done.',\n",
       " \"I'd like to tell you about one such child,\",\n",
       " 'This percentage is even greater than the percentage in India.',\n",
       " \"what we really mean is that they're bad at not paying attention.\",\n",
       " '.The ending portion of these Vedas is called Upanishad.',\n",
       " 'The then Governor of Kashmir resisted transfer , but was finally reduced to subjection with the aid of British .',\n",
       " 'In this lies the circumstances of people before you.',\n",
       " 'And who are we to say, even, that they are wrong',\n",
       " '“”Global Warming“” refer to warming caused in recent decades and probability of its continual presence and its indirect effect on human being.',\n",
       " \"You may want your child to go to a school that is not run by the LEA - a non-maintained special school or an independent school that can meet your child 's needs .\",\n",
       " 'Please ensure that you use the appropriate form .',\n",
       " 'Category: Religious Text',\n",
       " 'This period summarily is pepped up with devotion.',\n",
       " 'So there is some sort of justice',\n",
       " 'The first two were found unreliable and the prosecution case rested mainly on the evidence of the remaining five approvers .',\n",
       " \"They had justified their educational policy of concentrating on the education of a small number of upper and middle-class people with the argument that the new education would gradually ' filter down ' from above .\",\n",
       " 'And now at present the naturecure, Ayurvedic and modern treatments are taking place through the government in Nepal.',\n",
       " 'Parliament time frame is 5 years and this will be dissolved before that.',\n",
       " 'ii Register Courts , empowered to try causes for amounts not exceeding Rs 200 , when authorised by the judges .',\n",
       " 'Extreme weather due to increased mortality; displacements and economic loss will be compounded through growing population. Although, temperate climate has some benefits out of it such as decreased mortality due to cold weather.',\n",
       " 'Of these Lahadi is a popular one .',\n",
       " 'Even a concentration of 0.001 ppm of hydrogen sulphide in the water can emit the smell of rotten egg .',\n",
       " \"Islam is the world's second-largest religion, after Christianity.\",\n",
       " 'This changed slowly',\n",
       " 'Far more interesting are genetic diseases that arise essentially from the mutation of a single gene allowing a simple Mendelian distribution to appear in the offspring concerned .',\n",
       " 'The FIs are expected to offload the stake in favour of Suzuki -LRB- currently 50 per cent stakeholder -RRB- and to the general public later .',\n",
       " 'were being produced.',\n",
       " 'Naren had three or four meetings with the Consul but found that he was making no progress .',\n",
       " 'All those who stand today under this flag are Indians , not Hindus , not Muslims , but Indians .',\n",
       " 'In some cases the High Courts have the jurisdiction to deal with matters more effectively as courts of first instance than the Supreme Court .',\n",
       " 'And you can see, this LED is going to glow.',\n",
       " 'Under the Act more than one Claims Tribunal can be set up and their territorial jurisdiction is specified by notification constituting the Tribunals .',\n",
       " 'to turn on the lights or to bring him a glass of water,',\n",
       " 'Maine',\n",
       " 'IN farsi philosophy firdaus garden is presented as ideal one in Mugal literatures.',\n",
       " 'Can you imagine saying that?',\n",
       " 'Head lice need to maintain contact with a host in order to survive .',\n",
       " 'Three: this is a good road in - right near where our factory is located.',\n",
       " 'This may include injunctions , which can be highly effective preventive measures which can reduce the nuisance caused whilst allowing people to keep their homes .',\n",
       " \"What's going on?”\",\n",
       " 'But after his demise, his bother Sardar Vallabh Bhai Patel did not accept this will and filed a case in the court.',\n",
       " 'He also saw with great anguish how , following the ultra-left line of the sixth Comintern congress , the communists had destroyed themselves and gravely damaged the left movement .',\n",
       " 'There are also financial reforms in rural China.',\n",
       " 'That is the final lesson of the story .',\n",
       " 'It is a clash all along , of the old with the new , of real politik with idealism , of the means with-the end , of love claimed as of right and love given of free will , of home-bred virtue with the wild wind from the outside .',\n",
       " 'Progressive-minded , he advocated widow remarriage and raising the marriageable age of girls .',\n",
       " 'The range was comprehensive , the analysis sober and lucid , and the style so charming that what he said was itself afine specimen of literature .',\n",
       " 'Twenty of the years from 1860 to 1908 were years of famine .',\n",
       " 'This position is similar to armchair. In this the male puts its legs slightly up near the knees. and his hands are helpful to support the hips of the female. The female spreads her leg touching his back.and after letting her penis inside her vagina takes support on his arms by her hands.In this,he male and female remains quite closer and this provides quite closure.',\n",
       " 'the family planning started in Vietnam and they went for smaller families.',\n",
       " 'category:information technology',\n",
       " 'I mean, at that time, trust me,',\n",
       " 'For Jayendra Saraswati , the spiritual has never been static .',\n",
       " 'Not only that,',\n",
       " 'What we call instinct , what we always refer to disparagingly in our daily lifethis it is that leads us out of the jungle of virtues and vices , elations and afflictions , into an awareness of the unlimited possibilities of our destiny .',\n",
       " 'humans destroyed the commons that they depended on.',\n",
       " 'Almost goes to E, but otherwise the play would be over.',\n",
       " 'Aryans did not make any statues or temples for deities.',\n",
       " 'It has three small domes and has been constructed using white marble.',\n",
       " '.Sarojini Naidu with Mahatma Gandhi',\n",
       " 'But January 2002 has been quiet .',\n",
       " 'External links',\n",
       " \"On the rear wall , in a special niche , is carved the usual Somaskanda panel , with Siva and Uma seated with little Skanda on Uma 's lap and Brahma and Vishnu standing behind on either side .\",\n",
       " 'So I want to share with you a couple key insights',\n",
       " 'France , Belgium ; Germany , the United States , Russia , and later , Japan developed powerful industries and began to search for foreign markets for their products .',\n",
       " 'But if you agree to allow the seller extra time , you cannot cancel until that time is up .',\n",
       " 'Many countries in the [unclear], they need legitimacy.',\n",
       " 'This change has contributed',\n",
       " 'is to create ownership in the community to the problem,',\n",
       " 'Buddha and danceroom (1958)',\n",
       " \"the arrangements for setting short-term goals , regularly reviewing your child 's progress towards those goals , and how your child 's progress is to be monitored .\",\n",
       " 'He often spoke to us of his own great longing to meet him .',\n",
       " 'In this positive connotation , liberty would mean freedom of the individual to do what one likes .',\n",
       " \"Thus , even aslthe Central Legislature 's power to make laws for the whole of British India , for British subjects and servants of tie Crown in India , and for all British Indian subjects within as well as without British India was reiterated , it continued to be subject to many important limitations which were designed either to keep the sovereignty of the British Parliament intact or to maintain the supremacy of the Governor-General and his Council .\",\n",
       " 'greatly understate its impact.',\n",
       " 'We use a different style of WP1 application form depending on whether you intend to send the application to us by e-mail or post .',\n",
       " 'He wanted to teach gurubhai a lesson and giving respect to all the things of Gurudev ,he took the spitbowl filled with blood and cough and threw it on gurubhai bed.',\n",
       " \"1)Sanskrit is world's oldest language of vedas.\",\n",
       " 'the Carrier Hotel',\n",
       " 'that it learns or is pre-programmed.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d177bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है .',\n",
       " 'मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी,',\n",
       " 'यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।',\n",
       " 'हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते',\n",
       " 'इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।',\n",
       " 'कश्मीर के तत्कालीन गवर्नर ने इस हस्तांतरण का विरोध किया था , लेकिन अंग्रेजों की सहायता से उनकी आवाज दबा दी गयी .',\n",
       " 'इसमें तुमसे पूर्व गुज़रे हुए लोगों के हालात हैं।',\n",
       " 'और हम होते कौन हैं यह कहने भी वाले कि वे गलत हैं',\n",
       " 'ग्लोबल वॉर्मिंग से आशय हाल ही के दशकों में हुई वार्मिंग और इसके निरंतर बने रहने के अनुमान और इसके अप्रत्यक्ष रूप से मानव पर पड़ने वाले प्रभाव से है।',\n",
       " 'हो सकता है कि आप चाहते हों कि आप का नऋर्नमेनटेन्ड ह्यबिना किसी समर्थन के हृ विशेष स्कूल , या किसी स्वतंत्र स्कूल में जाए , इजसके पास विशेष शैक्षणिक जऋऋरतों वाले बच्चों के प्रति सहूलियत हों . .',\n",
       " 'कृपया यह सुनिश्चित कर लें कि आप सही फॉर्म का प्रयोग कर रहें हैं .',\n",
       " 'श्रेणी:धर्मग्रन्थ',\n",
       " 'यह काल समग्रतः भक्ति भावना से ओतप्रोत काल है।',\n",
       " 'तो वहाँ न्याय है',\n",
       " 'पहले दो को अविश्वसनीय मानकर बाकी पांच मुखबिरों के आधार पर मुकदमा चलाया गया .',\n",
       " 'कम संख़्या वाले उच्च एवं मध्यम श्रेणी के लोगों तक ही अपनी शिक्षा नीति को केंद्रित करने को इस तर्क के साथ न्यायसंगत बताया कि नयी शिक्षा Zक्रमश : ऊपर से नीचे की ओर छनते हुए जायेगी .',\n",
       " 'हाल में नेपाल के हस्पताल सामन्यतया आयुर्वेद, प्राकृतिक चिकित्सा तथा आधुनिक चिकीत्सा करके सरकारी सेवा विद्यमान हे ।',\n",
       " 'लोकसभा की कार्यावधि 5 वर्ष है पर्ंतु इसे समय से पूर्व भंग किया जा सकता है',\n",
       " 'रजिस्टर न्यायालय जिन्हें न्यायाधीश द्वारा प्राधिकृत किए जाने पर 200 रु . तक के वादों का निर्णय करने का अधिकार था .',\n",
       " 'बढ़ती हुई मौतों displacements और आर्थिक नुकसान जो की अतिवादी मौसम (extreme weather)के कारण संभावित हैं बढती हुई जनसँख्या (growing population)के कारण और भी बदतर हो सकते हैं . हालांकि शीतोष्ण क्षेत्र में इसके कुछ फैदे भी हो सकते हैं जैसे की ठंड की वजह से कम मौतें होना .',\n",
       " \"' लाडड़ी लोकप्रिय स्त्री नृत्य है .\",\n",
       " 'यहां तक कि पानी में हाइड्रोजन सल्फाइड की 0.001 पी पी एम मात्रा से भी सड़े हुए अंडे की बदबू आती है .',\n",
       " 'इस्लाम धर्म (الإسلام) ईसाई धर्म के बाद अनुयाइयों के आधार पर दुनिया का दूसरा सब से बड़ा धर्म है।',\n",
       " 'धीरे धीरे ये सब बदला',\n",
       " 'एक ही जीन के उत्परिवर्तन के कारण होने वाले आनुवंशिक रोग इन सभी रोगों से अधिक महत्वपूर्ण हैं.मेंडेल के सरल वितरण के नियम के अनुसार यह जीन संतानों में प्रकट होता है .',\n",
       " 'बाद में वित्तैइय संस्थाएं इन शेयरों को सुजुकी ( फिलहाल 50 प्रतिशत शेयरधारक ) और आम जनता को बेच देंगी .',\n",
       " 'उत्पन्न नहीं कि जाती थी.',\n",
       " 'नरेंद्र ने कौंसिल से तीन या चार मुलाकातें की किंतु उन्होनें महसूस किया कि वे किसी भी प्रकार से बात आगे नही बढ़ा पा रहे है .',\n",
       " 'जो भी इस झंडे के नीचे खड़ा है वह न हिंदू है , न मुसलमान , बल्कि वह हिंदुस्तानी है .',\n",
       " 'कुछ मामलों में प्रथम स्तर के न्यायालय होने के कारण उच्च न्यायालयों को उच्चतम न्यायालय की अपेक्षा मामलों पर अधिक कारगर ढंग से कार्रवाई करने की अधिकारिता है .',\n",
       " 'और जैसा आप देख रहे है, ये एल.ई.डी. जल उठेगी।',\n",
       " 'अधिनियम के अधीन एक से अधिक दावा अधिकरण स्थापित किए जा सकते हैं और उनकी अधिकारिता को अधिकरणों के गठन की अधिसूचना में विनिर्दिष्ट कर दिया जाता है .',\n",
       " 'लाईट जलाने के लिए या उनके लिए पानी लाने के लिए,',\n",
       " 'मेन',\n",
       " 'फारसी रहस्यवाद में मुगल कालीन इस्लामी पाठ्य में फिरदौस को एक आदर्श पूर्णता का बाग बताया गया है।',\n",
       " 'क्या आप ये कल्पना कर सकते है',\n",
       " 'जिंदा रहने के लिए जूँ किसी जीव के सम्पर्क में रहना आवश्यक है',\n",
       " 'तीसरी: ये हमारी फ़ैक्ट्री के पास की एक अपेक्षाकृत बेहतर सडक है।',\n",
       " 'इसमें कानूनी रोक ( इंजक्शन ) शामिल है जो बहुत ही प्रभावकारी रोक का उपाय है जो पैदा हुए उपद्रव को कम कर सकता है और इस बीच लोगों को अपने घरों में रहने की अनुमति भी होती है .',\n",
       " 'क्या हो रहा है ये?”',\n",
       " 'मगर उनके निधन के पश्चात उनके भाई सरदार वल्लभ भाई पटेल ने इस वसीयत को स्वीकार नहीं किया और उसपर अदालत में मुकदमा चलाया।',\n",
       " 'उन्होनें बड़े दुख के साथ यह भी देखा कि कैसे छठी कमिन्टर्न कांग्रेस की अतिवामपंथी मार्ग अपनाकर साम्यवादियों ने अपने को नष्ट कर लिया था और वामपंथी आंदोलन को भी बढ़ते देखा था .',\n",
       " 'ग्रामीण चीन में आर्थिक नवीनीकरण हुये हैं।',\n",
       " 'यह इस कहानी का अंतिम सबक है .',\n",
       " 'इसमें टकरवा ही टकराव है - नए के साथ पुराने का , राज्य सत्ता बनाम आदर्शवाद का , साध्य और साधन का , अधिकार पूर्ण प्रेम और प्रेम की स्वतंत्रता का , और बाहरी जंगली हवा के साथ ग्राहस्थिक या घरेलू मूल्यों का .',\n",
       " 'अपने प्रगतिशील विचारों के कारण उन्होंने विधवा-विवाह और कन्याओं की विवाह योग्य आयु-सीमा बढ़ाने की भी वकालत की .',\n",
       " 'विश्लेषण और विवेचन बड़े गंभीर और सरस थे और शैली इतनी मोहक थी कि सारा-का-सारा लेखन साहित्य का उत्कृष्ट निदर्शन बन गया .',\n",
       " 'सन् 1860 और 1908 के बीच के 20 वर्ष अकाल के वर्ष रहे .',\n",
       " '-झूला यह हत्थाकुर्सी से मिलती जुलती पोजीशन है इसमें पुरुष अपने पैरों को घुटनों के पास से थोड़ा उपर उठा लेता है और उसके हाथ महिला को कूल्हों के पास से सहारा देने का काम करते हैं तथा महिला अपने दोनों पैर उसकी कमर से सटाते हुए सीधे फैला लेती है तथा योनि को लिंग में प्रवेश कराने के बाद अपने हाथ उसके कंधे पर रख सहारा लेती है. इसमें महिला पुरुष काफी निकट रहते हैं इससे यह काफी निकटता प्रदान करती है.',\n",
       " 'वियतनाम में परिवार योजना शुरू हो गई और उनके परिवार छोटे होने लगे।',\n",
       " 'श्रेणी:सूचना प्रौद्योगिकी',\n",
       " 'मेरा मतलब, उस समय, सही मानिए,',\n",
       " 'जयेंद्र सरस्वती के लिए अध्यात्म कभी कोई स्थिर चीज नहीं रही है .',\n",
       " 'बस वही नहीं,',\n",
       " 'जिसे हम प्रवृत्ति कहते हैं और जिसे हम अपने दैनंदिन जीवन में बहुत तुच्छ समझ कर नकार दिया करते हैं- यही वह चीज है जो हमें सदगुणों या दुर्गुणों के अरण्य से बाहर निकालने में सहायक बनकर हमारी नियति की असीम संभावनाओं तक और हमारी चेतना एवं उल्लास तथा अनुताप तक ले जाती है .',\n",
       " 'मानवों ने उन ही साझे संसाधनों को नष्ट किया जिन पर वो आधारित थे।',\n",
       " 'रचना करीब करीब ई तक जाती है, मगर तब तो नाटक ख़त्म हो जाएगा.',\n",
       " 'आर्य देवताओं की कोई मूर्ति या मन्दिर नहीं बनाते थे।',\n",
       " 'यह एक छोटी तीन गुम्बद वाली तराशे हुए श्वेत संगमर्मर से निर्मित है।',\n",
       " 'महात्मा गांधी के साथ सरोजिनी नायडू',\n",
       " 'लेकिन जनवरी , 2002 में मंदी रही .',\n",
       " 'बाहरी कड़ियाँ',\n",
       " 'पिछली दीवार पर एक विशेष ताक में , एक सामान्य सोमस्कंद फल्क उकेरा गया है , जिसमें शिव और उमा बैठे हुए हैं , नन्हाँ स्कंद उमा की गोद में हैं और दोनों और ब्रह्मा और विष्णु खड़े हैं .',\n",
       " 'मैं आपके साथ कुछ मुख्य सूत्र बाँटना चाहता हूँ',\n",
       " 'फ्रांस , बेलजियम , जर्मनी , संयुक्त राज्य अमेरिका , रूस और बाद में जापान ने अपने यहां शक्तिशाली उद्योगों का विकास किया और अपने माल की खपत के लिए विदेशी बाजार की खोज शुरू की .',\n",
       " 'पर अगर आप व्यापारी को अधिक समय देना स्वीकार करते हैं , तो आप इस समय के बीत जाने के पहले अपना आर्डर रद्द नहीं कर सकते .',\n",
       " '[अस्पष्ट] के बहुत सारे राष्ट्रों को मान्यता चाहिए.',\n",
       " 'यह परिवर्तन योगदान दे रहा है',\n",
       " 'हमने समस्या का हल निकालने का ज़िम्मा पूरे समुदाय पर डाल दिया,',\n",
       " 'बुद्ध और नाचघर (1958)',\n",
       " 'अल्पकाली उद्देश्य रेखांकित करने का प्रबंध करना एवं उन उद्देश्यों के प्रति आप की नियमित उन्नति का निरीक्षण करें .',\n",
       " 'वे बहुधा उनसे मिलने की तीव्र इच्छा के बारे में भी हमें बताया करते .',\n",
       " 'इस सकारात्मक अर्थबोध में स्वतंत्रता का अर्थ होगा किसी व्यक्ति की अपनी इच्छा के अनुसार कार्य करने की स्वतंत्रता .',\n",
       " 'इस प्रकार , यद्यपि संपूर्ण ब्रिटिश इंडिया के लिए , भारत में सम्राट की ब्रिटिश प्रजा एवं सेवियों के लिए तथा ब्रिटिश इंडिया में और उससे बाहर ब्रिटिश इंडिया की सारी प्रजा के लिए केंद्रीय विधानमंडल की शक्ति को दोहराया गया परंतु उसकी अनेक महत्वपूर्ण सीमाएं बनी रहीं जो या तो ब्रिटिश संसद की प्रभुसत्ता ज्यों की त्यों बनाए रखने के लिए निर्धारित की गई थीं या Zइफर गवर्नर-जनरल तथा उसकी परिषद् की प्रभुसत्ता बनाए रखने के लिए .',\n",
       " 'इसके प्रभाव को बहुत ही कम आंका गया है।',\n",
       " 'हम भिऋ स्टाऋल के थ्फ् 1 आवेदनपत्र का प्रयोग करते हैं जो कि इस बात पर निर्भर करता है कि आप हमें आवेदनपत्र डऋआक से या इ-मेल से भेजना चाहते हैं .',\n",
       " 'उस गुरुभाई को पाठ पढ़ाते हुए और गुरुदेव की प्रत्येक वस्तु के प्रति प्रेम दर्शाते हुए उनके बिस्तर के पास रक्त कफ आदि से भरी थूकदानी उठाकर फेकते थे।',\n",
       " '१) संस्कृत, विश्व की सबसे पुरानी पुस्तक (वेद) की भाषा है।',\n",
       " 'कैरियर होटल',\n",
       " 'जो वह सीखता है या पूर्व क्रमादेशित होता है|']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66dfb411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"factory=IndicNormalizerFactory()\\nnormalizer=factory.get_normalizer('hi',remove_nuktas = True)\\ncounterx = Counter()\\nfor x in target_train_file:\\n    #print(x)\\n    words = normalizer.normalize(x)\\n    #tokens = indic_tokenize.trivial_tokenize(words)\\n    counterx.update(words)\\nprint(counterx)\\n#print(vocab(counterx))\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"factory=IndicNormalizerFactory()\n",
    "normalizer=factory.get_normalizer('hi',remove_nuktas = True)\n",
    "counterx = Counter()\n",
    "for x in target_train_file:\n",
    "    #print(x)\n",
    "    words = normalizer.normalize(x)\n",
    "    #tokens = indic_tokenize.trivial_tokenize(words)\n",
    "    counterx.update(words)\n",
    "print(counterx)\n",
    "#print(vocab(counterx))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39900b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = get_tokenizer('spacy', language = 'en_core_web_sm')\n",
    "def hi_tokenizer(data):\n",
    "    factory=IndicNormalizerFactory()\n",
    "    normalizer=factory.get_normalizer('hi',remove_nuktas = True)\n",
    "    text = normalizer.normalize(data)\n",
    "    words = indic_tokenize.trivial_tokenize(text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a50c03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(filedata, tokenizer):\n",
    "    counter = Counter()\n",
    "    for string_ in filedata:\n",
    "        #print(string_)\n",
    "        counter.update(tokenizer(string_))\n",
    "    print(counter)\n",
    "    return(vocab(counter, specials = ['<unk>', '<pad>', '<bos>', '<eos>']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51e8d373",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 74, '.': 66, ',': 55, 'of': 45, 'to': 44, 'and': 35, 'is': 24, 'in': 17, 'that': 17, 'with': 16, 'a': 16, '-': 13, 'on': 11, 'this': 10, 'not': 9, 'you': 9, 'for': 9, 'be': 8, 'are': 8, \"'s\": 8, 'or': 7, 'can': 7, 'his': 7, 'This': 6, 'British': 6, 'has': 6, 'as': 6, 'what': 5, 'one': 5, 'child': 5, 'they': 5, 'The': 5, 'was': 5, 'In': 5, 'by': 5, 'it': 5, 'he': 5, 'India': 4, 'we': 4, 'but': 4, 'its': 4, 'your': 4, 'up': 4, 'were': 4, 'their': 4, 'from': 4, 'time': 4, 'will': 4, 'her': 4, 'do': 3, 'I': 3, 'even': 3, 'than': 3, 'mean': 3, 'at': 3, 'people': 3, 'And': 3, '”': 3, 'school': 3, ':': 3, 'some': 3, 'had': 3, 'would': 3, 'years': 3, 'after': 3, 'more': 3, 'progress': 3, 'our': 3, 'which': 3, 'But': 3, 'He': 3, 'all': 3, 'female': 3, 'been': 3, 'us': 3, 'have': 2, 'needs': 2, 'such': 2, 'percentage': 2, 'these': 2, 'Governor': 2, 'before': 2, 'who': 2, '“': 2, 'refer': 2, 'caused': 2, 'being': 2, 'may': 2, 'want': 2, 'special': 2, 'an': 2, 'meet': 2, 'use': 2, 'form': 2, 'So': 2, 'first': 2, 'found': 2, 'case': 2, 'education': 2, 'small': 2, 'new': 2, \"'\": 2, 'through': 2, 'Parliament': 2, 'Courts': 2, 'weather': 2, 'due': 2, 'mortality': 2, ';': 2, 'out': 2, 'water': 2, 'world': 2, 'allowing': 2, 'later': 2, 'three': 2, 'those': 2, 'Indians': 2, 'jurisdiction': 2, 'going': 2, 'him': 2, '?': 2, 'need': 2, 'maintain': 2, 'right': 2, 'near': 2, 'keep': 2, 'What': 2, 'did': 2, 'also': 2, 'great': 2, 'how': 2, 'left': 2, 'destroyed': 2, 'lesson': 2, 'It': 2, 'love': 2, 'style': 2, 'male': 2, 'hands': 2, 'support': 2, 'quite': 2, 'make': 2, 'Uma': 2, 'either': 2, 'goals': 2, 'subjects': 2, 'application': 2, 'gurubhai': 2, 'politicians': 1, 'permission': 1, 'done': 1, \"'d\": 1, 'like': 1, 'tell': 1, 'about': 1, 'greater': 1, 'really': 1, \"'re\": 1, 'bad': 1, 'paying': 1, 'attention': 1, '.The': 1, 'ending': 1, 'portion': 1, 'Vedas': 1, 'called': 1, 'Upanishad': 1, 'then': 1, 'Kashmir': 1, 'resisted': 1, 'transfer': 1, 'finally': 1, 'reduced': 1, 'subjection': 1, 'aid': 1, 'lies': 1, 'circumstances': 1, 'say': 1, 'wrong': 1, 'Global': 1, 'Warming': 1, 'warming': 1, 'recent': 1, 'decades': 1, 'probability': 1, 'continual': 1, 'presence': 1, 'indirect': 1, 'effect': 1, 'human': 1, 'You': 1, 'go': 1, 'run': 1, 'LEA': 1, 'non': 1, 'maintained': 1, 'independent': 1, 'Please': 1, 'ensure': 1, 'appropriate': 1, 'Category': 1, 'Religious': 1, 'Text': 1, 'period': 1, 'summarily': 1, 'pepped': 1, 'devotion': 1, 'there': 1, 'sort': 1, 'justice': 1, 'two': 1, 'unreliable': 1, 'prosecution': 1, 'rested': 1, 'mainly': 1, 'evidence': 1, 'remaining': 1, 'five': 1, 'approvers': 1, 'They': 1, 'justified': 1, 'educational': 1, 'policy': 1, 'concentrating': 1, 'number': 1, 'upper': 1, 'middle': 1, 'class': 1, 'argument': 1, 'gradually': 1, 'filter': 1, 'down': 1, 'above': 1, 'now': 1, 'present': 1, 'naturecure': 1, 'Ayurvedic': 1, 'modern': 1, 'treatments': 1, 'taking': 1, 'place': 1, 'government': 1, 'Nepal': 1, 'frame': 1, '5': 1, 'dissolved': 1, 'ii': 1, 'Register': 1, 'empowered': 1, 'try': 1, 'causes': 1, 'amounts': 1, 'exceeding': 1, 'Rs': 1, '200': 1, 'when': 1, 'authorised': 1, 'judges': 1, 'Extreme': 1, 'increased': 1, 'displacements': 1, 'economic': 1, 'loss': 1, 'compounded': 1, 'growing': 1, 'population': 1, 'Although': 1, 'temperate': 1, 'climate': 1, 'benefits': 1, 'decreased': 1, 'cold': 1, 'Of': 1, 'Lahadi': 1, 'popular': 1, 'Even': 1, 'concentration': 1, '0.001': 1, 'ppm': 1, 'hydrogen': 1, 'sulphide': 1, 'emit': 1, 'smell': 1, 'rotten': 1, 'egg': 1, 'Islam': 1, 'second': 1, 'largest': 1, 'religion': 1, 'Christianity': 1, 'changed': 1, 'slowly': 1, 'Far': 1, 'interesting': 1, 'genetic': 1, 'diseases': 1, 'arise': 1, 'essentially': 1, 'mutation': 1, 'single': 1, 'gene': 1, 'simple': 1, 'Mendelian': 1, 'distribution': 1, 'appear': 1, 'offspring': 1, 'concerned': 1, 'FIs': 1, 'expected': 1, 'offload': 1, 'stake': 1, 'favour': 1, 'Suzuki': 1, '-LRB-': 1, 'currently': 1, '50': 1, 'per': 1, 'cent': 1, 'stakeholder': 1, '-RRB-': 1, 'general': 1, 'public': 1, 'produced': 1, 'Naren': 1, 'four': 1, 'meetings': 1, 'Consul': 1, 'making': 1, 'no': 1, 'All': 1, 'stand': 1, 'today': 1, 'under': 1, 'flag': 1, 'Hindus': 1, 'Muslims': 1, 'cases': 1, 'High': 1, 'deal': 1, 'matters': 1, 'effectively': 1, 'courts': 1, 'instance': 1, 'Supreme': 1, 'Court': 1, 'see': 1, 'LED': 1, 'glow': 1, 'Under': 1, 'Act': 1, 'Claims': 1, 'Tribunal': 1, 'set': 1, 'territorial': 1, 'specified': 1, 'notification': 1, 'constituting': 1, 'Tribunals': 1, 'turn': 1, 'lights': 1, 'bring': 1, 'glass': 1, 'Maine': 1, 'IN': 1, 'farsi': 1, 'philosophy': 1, 'firdaus': 1, 'garden': 1, 'presented': 1, 'ideal': 1, 'Mugal': 1, 'literatures': 1, 'Can': 1, 'imagine': 1, 'saying': 1, 'Head': 1, 'lice': 1, 'contact': 1, 'host': 1, 'order': 1, 'survive': 1, 'Three': 1, 'good': 1, 'road': 1, 'where': 1, 'factory': 1, 'located': 1, 'include': 1, 'injunctions': 1, 'highly': 1, 'effective': 1, 'preventive': 1, 'measures': 1, 'reduce': 1, 'nuisance': 1, 'whilst': 1, 'homes': 1, 'demise': 1, 'bother': 1, 'Sardar': 1, 'Vallabh': 1, 'Bhai': 1, 'Patel': 1, 'accept': 1, 'filed': 1, 'court': 1, 'saw': 1, 'anguish': 1, 'following': 1, 'ultra': 1, 'line': 1, 'sixth': 1, 'Comintern': 1, 'congress': 1, 'communists': 1, 'themselves': 1, 'gravely': 1, 'damaged': 1, 'movement': 1, 'There': 1, 'financial': 1, 'reforms': 1, 'rural': 1, 'China': 1, 'That': 1, 'final': 1, 'story': 1, 'clash': 1, 'along': 1, 'old': 1, 'real': 1, 'politik': 1, 'idealism': 1, 'means': 1, 'end': 1, 'claimed': 1, 'given': 1, 'free': 1, 'home': 1, 'bred': 1, 'virtue': 1, 'wild': 1, 'wind': 1, 'outside': 1, 'Progressive': 1, 'minded': 1, 'advocated': 1, 'widow': 1, 'remarriage': 1, 'raising': 1, 'marriageable': 1, 'age': 1, 'girls': 1, 'range': 1, 'comprehensive': 1, 'analysis': 1, 'sober': 1, 'lucid': 1, 'so': 1, 'charming': 1, 'said': 1, 'itself': 1, 'afine': 1, 'specimen': 1, 'literature': 1, 'Twenty': 1, '1860': 1, '1908': 1, 'famine': 1, 'position': 1, 'similar': 1, 'armchair': 1, 'puts': 1, 'legs': 1, 'slightly': 1, 'knees': 1, 'helpful': 1, 'hips': 1, 'spreads': 1, 'leg': 1, 'touching': 1, 'back.and': 1, 'letting': 1, 'penis': 1, 'inside': 1, 'vagina': 1, 'takes': 1, 'arms': 1, 'remains': 1, 'closer': 1, 'provides': 1, 'closure': 1, 'family': 1, 'planning': 1, 'started': 1, 'Vietnam': 1, 'went': 1, 'smaller': 1, 'families': 1, 'category': 1, 'information': 1, 'technology': 1, 'trust': 1, 'me': 1, 'For': 1, 'Jayendra': 1, 'Saraswati': 1, 'spiritual': 1, 'never': 1, 'static': 1, 'Not': 1, 'only': 1, 'call': 1, 'instinct': 1, 'always': 1, 'disparagingly': 1, 'daily': 1, 'lifethis': 1, 'leads': 1, 'jungle': 1, 'virtues': 1, 'vices': 1, 'elations': 1, 'afflictions': 1, 'into': 1, 'awareness': 1, 'unlimited': 1, 'possibilities': 1, 'destiny': 1, 'humans': 1, 'commons': 1, 'depended': 1, 'Almost': 1, 'goes': 1, 'E': 1, 'otherwise': 1, 'play': 1, 'over': 1, 'Aryans': 1, 'any': 1, 'statues': 1, 'temples': 1, 'deities': 1, 'domes': 1, 'constructed': 1, 'using': 1, 'white': 1, 'marble': 1, '.Sarojini': 1, 'Naidu': 1, 'Mahatma': 1, 'Gandhi': 1, 'January': 1, '2002': 1, 'quiet': 1, 'External': 1, 'links': 1, 'On': 1, 'rear': 1, 'wall': 1, 'niche': 1, 'carved': 1, 'usual': 1, 'Somaskanda': 1, 'panel': 1, 'Siva': 1, 'seated': 1, 'little': 1, 'Skanda': 1, 'lap': 1, 'Brahma': 1, 'Vishnu': 1, 'standing': 1, 'behind': 1, 'side': 1, 'share': 1, 'couple': 1, 'key': 1, 'insights': 1, 'France': 1, 'Belgium': 1, 'Germany': 1, 'United': 1, 'States': 1, 'Russia': 1, 'Japan': 1, 'developed': 1, 'powerful': 1, 'industries': 1, 'began': 1, 'search': 1, 'foreign': 1, 'markets': 1, 'products': 1, 'if': 1, 'agree': 1, 'allow': 1, 'seller': 1, 'extra': 1, 'cancel': 1, 'until': 1, 'Many': 1, 'countries': 1, '[': 1, 'unclear': 1, ']': 1, 'legitimacy': 1, 'change': 1, 'contributed': 1, 'create': 1, 'ownership': 1, 'community': 1, 'problem': 1, 'Buddha': 1, 'danceroom': 1, '(': 1, '1958': 1, ')': 1, 'arrangements': 1, 'setting': 1, 'short': 1, 'term': 1, 'regularly': 1, 'reviewing': 1, 'towards': 1, 'monitored': 1, 'often': 1, 'spoke': 1, 'own': 1, 'longing': 1, 'positive': 1, 'connotation': 1, 'liberty': 1, 'freedom': 1, 'individual': 1, 'likes': 1, 'Thus': 1, 'aslthe': 1, 'Central': 1, 'Legislature': 1, 'power': 1, 'laws': 1, 'whole': 1, 'servants': 1, 'tie': 1, 'Crown': 1, 'Indian': 1, 'within': 1, 'well': 1, 'without': 1, 'reiterated': 1, 'continued': 1, 'subject': 1, 'many': 1, 'important': 1, 'limitations': 1, 'designed': 1, 'sovereignty': 1, 'intact': 1, 'supremacy': 1, 'General': 1, 'Council': 1, 'greatly': 1, 'understate': 1, 'impact': 1, 'We': 1, 'different': 1, 'WP1': 1, 'depending': 1, 'whether': 1, 'intend': 1, 'send': 1, 'e': 1, 'mail': 1, 'post': 1, 'wanted': 1, 'teach': 1, 'giving': 1, 'respect': 1, 'things': 1, 'Gurudev': 1, 'took': 1, 'spitbowl': 1, 'filled': 1, 'blood': 1, 'cough': 1, 'threw': 1, 'bed': 1, '1)Sanskrit': 1, 'oldest': 1, 'language': 1, 'vedas': 1, 'Carrier': 1, 'Hotel': 1, 'learns': 1, 'pre': 1, 'programmed': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab = build_vocab(source_train_file,en_tokenizer)\n",
    "en_vocab.set_default_index(en_vocab['<unk>'])\n",
    "en_vocab[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e3ecc3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'के': 61, 'है': 48, '.': 46, 'की': 39, ',': 32, 'और': 32, 'में': 29, 'से': 25, 'का': 25, 'को': 22, '।': 20, 'हैं': 20, 'कि': 14, 'यह': 11, 'भी': 11, 'पर': 11, 'लिए': 11, 'इस': 10, 'या': 10, 'जो': 9, 'नहीं': 9, 'हुए': 9, 'आप': 9, 'अपने': 9, '-': 9, 'ही': 8, 'एक': 8, 'कर': 8, ')': 8, '(': 7, 'पास': 6, 'करने': 6, 'ये': 6, 'ने': 6, 'किया': 6, 'हो': 6, 'गया': 6, 'तक': 6, 'साथ': 6, 'तथा': 6, 'वह': 5, 'अधिक': 5, 'हम': 5, 'इसमें': 5, 'वाले': 5, 'किसी': 5, 'कारण': 5, 'सकते': 5, 'उनके': 5, 'करते': 5, 'ब्रिटिश': 5, 'था': 4, 'इसके': 4, 'तो': 4, 'कम': 4, 'एवं': 4, 'समय': 4, 'बाद': 4, 'बहुत': 4, 'थे': 4, 'प्रतिशत': 3, 'चाहते': 3, 'पूर्व': 3, 'लोगों': 3, 'वे': 3, 'हुई': 3, 'रहने': 3, 'सकता': 3, 'विशेष': 3, 'प्रति': 3, 'बताया': 3, 'वर्ष': 3, 'न्यायालय': 3, 'कुछ': 3, 'धर्म': 3, 'होने': 3, 'जाती': 3, 'रहे': 3, 'दिया': 3, 'हमारी': 3, 'प्रेम': 3, 'स्वतंत्रता': 3, 'महिला': 3, 'उसकी': 3, 'हमें': 3, 'इंडिया': 3, 'कार्य': 2, 'करना': 2, 'चाहिए': 2, 'अनुमति': 2, 'बारे': 2, 'भारत': 2, 'वो': 2, 'दे': 2, 'अंतिम': 2, 'गवर्नर': 2, 'लेकिन': 2, 'उनकी': 2, 'हाल': 2, 'प्रभाव': 2, 'हों': 2, 'स्कूल': 2, 'सही': 2, 'प्रयोग': 2, 'काल': 2, 'पहले': 2, 'आधार': 2, 'मुकदमा': 2, 'चलाया': 2, 'उच्च': 2, 'अपनी': 2, 'शिक्षा': 2, 'नीचे': 2, 'जा': 2, 'किए': 2, 'जाने': 2, 'अधिकार': 2, 'बढती': 2, 'आर्थिक': 2, 'यहां': 2, 'पानी': 2, 'पी': 2, 'सब': 2, 'धीरे': 2, 'जीन': 2, 'इन': 2, 'महत्वपूर्ण': 2, 'अनुसार': 2, 'होता': 2, 'थी': 2, 'तीन': 2, 'उन्होनें': 2, 'प्रकार': 2, 'बात': 2, 'न': 2, 'मामलों': 2, 'अधिकारिता': 2, 'ई': 2, 'क्या': 2, 'रोक': 2, 'बीच': 2, 'रहा': 2, 'मगर': 2, 'भाई': 2, 'स्वीकार': 2, 'बडे': 2, 'देखा': 2, 'नष्ट': 2, 'राज्य': 2, 'बाहरी': 2, 'विवाह': 2, 'सारा': 2, 'पुरुष': 2, 'उसके': 2, 'हाथ': 2, 'सहारा': 2, 'दोनों': 2, 'लेती': 2, 'काफी': 2, 'परिवार': 2, 'शुरू': 2, 'गई': 2, 'उस': 2, 'कोई': 2, 'चीज': 2, 'रही': 2, 'जिसे': 2, 'बाहर': 2, 'निकालने': 2, 'उन': 2, 'करीब': 2, 'उमा': 2, 'इच्छा': 2, 'प्रजा': 2, 'प्रभुसत्ता': 2, 'बनाए': 2, 'रखने': 2, 'आवेदनपत्र': 2, 'राजनीतिज्ञों': 1, 'मई': 1, 'आपको': 1, 'ऐसे': 1, 'बच्चे': 1, 'बताना': 1, 'चाहूंगी': 1, 'हिन्दुओं': 1, 'कहना': 1, 'ध्यान': 1, 'पाते': 1, 'इन्हीं': 1, 'वेदों': 1, 'भाग': 1, 'उपनिषद': 1, 'कहलाता': 1, 'कश्मीर': 1, 'तत्कालीन': 1, 'हस्तांतरण': 1, 'विरोध': 1, 'अंग्रेजों': 1, 'सहायता': 1, 'आवाज': 1, 'दबा': 1, 'दी': 1, 'गयी': 1, 'तुमसे': 1, 'गुजरे': 1, 'हालात': 1, 'होते': 1, 'कौन': 1, 'कहने': 1, 'गलत': 1, 'ग्लोबल': 1, 'वॉर्मिंग': 1, 'आशय': 1, 'दशकों': 1, 'वार्मिंग': 1, 'निरंतर': 1, 'बने': 1, 'अनुमान': 1, 'अप्रत्यक्ष': 1, 'रूप': 1, 'मानव': 1, 'पडने': 1, 'नऋर्नमेनटेन्ड': 1, 'ह्यबिना': 1, 'समर्थन': 1, 'हृ': 1, 'स्वतंत्र': 1, 'जाए': 1, 'इजसके': 1, 'शैक्षणिक': 1, 'जऋऋरतों': 1, 'बच्चों': 1, 'सहूलियत': 1, 'कृपया': 1, 'सुनिश्चित': 1, 'लें': 1, 'फॉर्म': 1, 'रहें': 1, 'श्रेणीःधर्मग्रन्थ': 1, 'समग्रतः': 1, 'भक्ति': 1, 'भावना': 1, 'ओतप्रोत': 1, 'वहाँ': 1, 'न्याय': 1, 'दो': 1, 'अविश्वसनीय': 1, 'मानकर': 1, 'बाकी': 1, 'पांच': 1, 'मुखबिरों': 1, 'संख्या': 1, 'मध्यम': 1, 'श्रेणी': 1, 'नीति': 1, 'केंद्रित': 1, 'तर्क': 1, 'न्यायसंगत': 1, 'नयी': 1, 'Zक्रमश': 1, ':': 1, 'ऊपर': 1, 'ओर': 1, 'छनते': 1, 'जायेगी': 1, 'नेपाल': 1, 'हस्पताल': 1, 'सामन्यतया': 1, 'आयुर्वेद': 1, 'प्राकृतिक': 1, 'चिकित्सा': 1, 'आधुनिक': 1, 'चिकीत्सा': 1, 'करके': 1, 'सरकारी': 1, 'सेवा': 1, 'विद्यमान': 1, 'हे': 1, 'लोकसभा': 1, 'कार्यावधि': 1, '5': 1, 'पर्ंतु': 1, 'इसे': 1, 'भंग': 1, 'रजिस्टर': 1, 'जिन्हें': 1, 'न्यायाधीश': 1, 'द्वारा': 1, 'प्राधिकृत': 1, '200': 1, 'रु': 1, 'वादों': 1, 'निर्णय': 1, 'मौतों': 1, 'displacements': 1, 'नुकसान': 1, 'अतिवादी': 1, 'मौसम': 1, 'extreme': 1, 'weather': 1, 'संभावित': 1, 'जनसँख्या': 1, 'growing': 1, 'population': 1, 'बदतर': 1, 'हालांकि': 1, 'शीतोष्ण': 1, 'क्षेत्र': 1, 'फैदे': 1, 'जैसे': 1, 'ठंड': 1, 'वजह': 1, 'मौतें': 1, 'होना': 1, \"'\": 1, 'लाडडी': 1, 'लोकप्रिय': 1, 'स्त्री': 1, 'नृत्य': 1, 'हाइड्रोजन': 1, 'सल्फाइड': 1, '0.001': 1, 'एम': 1, 'मात्रा': 1, 'सडे': 1, 'अंडे': 1, 'बदबू': 1, 'आती': 1, 'इस्लाम': 1, 'الإسلام': 1, 'ईसाई': 1, 'अनुयाइयों': 1, 'दुनिया': 1, 'दूसरा': 1, 'बडा': 1, 'बदला': 1, 'उत्परिवर्तन': 1, 'आनुवंशिक': 1, 'रोग': 1, 'सभी': 1, 'रोगों': 1, 'मेंडेल': 1, 'सरल': 1, 'वितरण': 1, 'नियम': 1, 'संतानों': 1, 'प्रकट': 1, 'वित्तैइय': 1, 'संस्थाएं': 1, 'शेयरों': 1, 'सुजुकी': 1, 'फिलहाल': 1, '50': 1, 'शेयरधारक': 1, 'आम': 1, 'जनता': 1, 'बेच': 1, 'देंगी': 1, 'उत्पन्न': 1, 'नरेंद्र': 1, 'कौंसिल': 1, 'चार': 1, 'मुलाकातें': 1, 'किंतु': 1, 'महसूस': 1, 'आगे': 1, 'नही': 1, 'बढा': 1, 'पा': 1, 'झंडे': 1, 'खडा': 1, 'हिंदू': 1, 'मुसलमान': 1, 'बल्कि': 1, 'हिंदुस्तानी': 1, 'प्रथम': 1, 'स्तर': 1, 'न्यायालयों': 1, 'उच्चतम': 1, 'अपेक्षा': 1, 'कारगर': 1, 'ढंग': 1, 'कार्रवाई': 1, 'जैसा': 1, 'देख': 1, 'एल': 1, 'डी': 1, 'जल': 1, 'उठेगी': 1, 'अधिनियम': 1, 'अधीन': 1, 'दावा': 1, 'अधिकरण': 1, 'स्थापित': 1, 'अधिकरणों': 1, 'गठन': 1, 'अधिसूचना': 1, 'विनिर्दिष्ट': 1, 'जाता': 1, 'लाईट': 1, 'जलाने': 1, 'लाने': 1, 'मेन': 1, 'फारसी': 1, 'रहस्यवाद': 1, 'मुगल': 1, 'कालीन': 1, 'इस्लामी': 1, 'पाठ्य': 1, 'फिरदौस': 1, 'आदर्श': 1, 'पूर्णता': 1, 'बाग': 1, 'कल्पना': 1, 'जिंदा': 1, 'जूँ': 1, 'जीव': 1, 'सम्पर्क': 1, 'रहना': 1, 'आवश्यक': 1, 'तीसरीः': 1, 'फैक्ट्री': 1, 'अपेक्षाकृत': 1, 'बेहतर': 1, 'सडक': 1, 'कानूनी': 1, 'इंजक्शन': 1, 'शामिल': 1, 'प्रभावकारी': 1, 'उपाय': 1, 'पैदा': 1, 'उपद्रव': 1, 'घरों': 1, 'होती': 1, '?': 1, '\"': 1, 'निधन': 1, 'पश्चात': 1, 'सरदार': 1, 'वल्लभ': 1, 'पटेल': 1, 'वसीयत': 1, 'उसपर': 1, 'अदालत': 1, 'दुख': 1, 'कैसे': 1, 'छठी': 1, 'कमिन्टर्न': 1, 'कांग्रेस': 1, 'अतिवामपंथी': 1, 'मार्ग': 1, 'अपनाकर': 1, 'साम्यवादियों': 1, 'लिया': 1, 'वामपंथी': 1, 'आंदोलन': 1, 'बढते': 1, 'ग्रामीण': 1, 'चीन': 1, 'नवीनीकरण': 1, 'हुये': 1, 'कहानी': 1, 'सबक': 1, 'टकरवा': 1, 'टकराव': 1, 'नए': 1, 'पुराने': 1, 'सत्ता': 1, 'बनाम': 1, 'आदर्शवाद': 1, 'साध्य': 1, 'साधन': 1, 'पूर्ण': 1, 'जंगली': 1, 'हवा': 1, 'ग्राहस्थिक': 1, 'घरेलू': 1, 'मूल्यों': 1, 'प्रगतिशील': 1, 'विचारों': 1, 'उन्होंने': 1, 'विधवा': 1, 'कन्याओं': 1, 'योग्य': 1, 'आयु': 1, 'सीमा': 1, 'बढाने': 1, 'वकालत': 1, 'विश्लेषण': 1, 'विवेचन': 1, 'गंभीर': 1, 'सरस': 1, 'शैली': 1, 'इतनी': 1, 'मोहक': 1, 'लेखन': 1, 'साहित्य': 1, 'उत्कृष्ट': 1, 'निदर्शन': 1, 'बन': 1, 'सन्': 1, '1860': 1, '1908': 1, '20': 1, 'अकाल': 1, 'झूला': 1, 'हत्थाकुर्सी': 1, 'मिलती': 1, 'जुलती': 1, 'पोजीशन': 1, 'पैरों': 1, 'घुटनों': 1, 'थोडा': 1, 'उपर': 1, 'उठा': 1, 'लेता': 1, 'कूल्हों': 1, 'देने': 1, 'काम': 1, 'पैर': 1, 'कमर': 1, 'सटाते': 1, 'सीधे': 1, 'फैला': 1, 'योनि': 1, 'लिंग': 1, 'प्रवेश': 1, 'कराने': 1, 'कंधे': 1, 'रख': 1, 'निकट': 1, 'रहते': 1, 'इससे': 1, 'निकटता': 1, 'प्रदान': 1, 'करती': 1, 'वियतनाम': 1, 'योजना': 1, 'छोटे': 1, 'लगे': 1, 'श्रेणीःसूचना': 1, 'प्रौद्योगिकी': 1, 'मेरा': 1, 'मतलब': 1, 'मानिए': 1, 'जयेंद्र': 1, 'सरस्वती': 1, 'अध्यात्म': 1, 'कभी': 1, 'स्थिर': 1, 'बस': 1, 'वही': 1, 'प्रवृत्ति': 1, 'कहते': 1, 'दैनंदिन': 1, 'जीवन': 1, 'तुच्छ': 1, 'समझ': 1, 'नकार': 1, 'यही': 1, 'सदगुणों': 1, 'दुर्गुणों': 1, 'अरण्य': 1, 'सहायक': 1, 'बनकर': 1, 'नियति': 1, 'असीम': 1, 'संभावनाओं': 1, 'चेतना': 1, 'उल्लास': 1, 'अनुताप': 1, 'ले': 1, 'मानवों': 1, 'साझे': 1, 'संसाधनों': 1, 'जिन': 1, 'आधारित': 1, 'रचना': 1, 'तब': 1, 'नाटक': 1, 'खत्म': 1, 'जाएगा': 1, 'आर्य': 1, 'देवताओं': 1, 'मूर्ति': 1, 'मन्दिर': 1, 'बनाते': 1, 'छोटी': 1, 'गुम्बद': 1, 'वाली': 1, 'तराशे': 1, 'श्वेत': 1, 'संगमर्मर': 1, 'निर्मित': 1, 'महात्मा': 1, 'गांधी': 1, 'सरोजिनी': 1, 'नायडू': 1, 'जनवरी': 1, '2002': 1, 'मंदी': 1, 'कडियाँ': 1, 'पिछली': 1, 'दीवार': 1, 'ताक': 1, 'सामान्य': 1, 'सोमस्कंद': 1, 'फल्क': 1, 'उकेरा': 1, 'जिसमें': 1, 'शिव': 1, 'बैठे': 1, 'नन्हाँ': 1, 'स्कंद': 1, 'गोद': 1, 'ब्रह्मा': 1, 'विष्णु': 1, 'खडे': 1, 'मैं': 1, 'आपके': 1, 'मुख्य': 1, 'सूत्र': 1, 'बाँटना': 1, 'चाहता': 1, 'हूँ': 1, 'फ्रांस': 1, 'बेलजियम': 1, 'जर्मनी': 1, 'संयुक्त': 1, 'अमेरिका': 1, 'रूस': 1, 'जापान': 1, 'शक्तिशाली': 1, 'उद्योगों': 1, 'विकास': 1, 'माल': 1, 'खपत': 1, 'विदेशी': 1, 'बाजार': 1, 'खोज': 1, 'अगर': 1, 'व्यापारी': 1, 'देना': 1, 'बीत': 1, 'अपना': 1, 'आर्डर': 1, 'रद्द': 1, '[': 1, 'अस्पष्ट': 1, ']': 1, 'सारे': 1, 'राष्ट्रों': 1, 'मान्यता': 1, 'परिवर्तन': 1, 'योगदान': 1, 'हमने': 1, 'समस्या': 1, 'हल': 1, 'जिम्मा': 1, 'पूरे': 1, 'समुदाय': 1, 'डाल': 1, 'बुद्ध': 1, 'नाचघर': 1, '1958': 1, 'अल्पकाली': 1, 'उद्देश्य': 1, 'रेखांकित': 1, 'प्रबंध': 1, 'उद्देश्यों': 1, 'नियमित': 1, 'उन्नति': 1, 'निरीक्षण': 1, 'करें': 1, 'बहुधा': 1, 'उनसे': 1, 'मिलने': 1, 'तीव्र': 1, 'सकारात्मक': 1, 'अर्थबोध': 1, 'अर्थ': 1, 'होगा': 1, 'व्यक्ति': 1, 'यद्यपि': 1, 'संपूर्ण': 1, 'सम्राट': 1, 'सेवियों': 1, 'उससे': 1, 'सारी': 1, 'केंद्रीय': 1, 'विधानमंडल': 1, 'शक्ति': 1, 'दोहराया': 1, 'परंतु': 1, 'अनेक': 1, 'सीमाएं': 1, 'बनी': 1, 'रहीं': 1, 'संसद': 1, 'ज्यों': 1, 'त्यों': 1, 'निर्धारित': 1, 'थीं': 1, 'Zइफर': 1, 'जनरल': 1, 'परिषद्': 1, 'आंका': 1, 'भिऋ': 1, 'स्टाऋल': 1, 'थ्फ्': 1, '1': 1, 'निर्भर': 1, 'करता': 1, 'डऋआक': 1, 'इ': 1, 'मेल': 1, 'भेजना': 1, 'गुरुभाई': 1, 'पाठ': 1, 'पढाते': 1, 'गुरुदेव': 1, 'प्रत्येक': 1, 'वस्तु': 1, 'दर्शाते': 1, 'बिस्तर': 1, 'रक्त': 1, 'कफ': 1, 'आदि': 1, 'भरी': 1, 'थूकदानी': 1, 'उठाकर': 1, 'फेकते': 1, '१': 1, 'संस्कृत': 1, 'विश्व': 1, 'सबसे': 1, 'पुरानी': 1, 'पुस्तक': 1, 'वेद': 1, 'भाषा': 1, 'कैरियर': 1, 'होटल': 1, 'सीखता': 1, 'क्रमादेशित': 1})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hi_vocab = build_vocab(target_train_file,hi_tokenizer)\n",
    "hi_vocab.set_default_index(hi_vocab['<unk>'])\n",
    "hi_vocab['<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ad46ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab['beautiful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e2b1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(filepath_source,filepath_target):\n",
    "    raw_en_iter = iter(filepath_source)\n",
    "    raw_hi_iter = iter(filepath_target)\n",
    "    data = []\n",
    "    for (raw_en, raw_hi) in zip(raw_en_iter, raw_hi_iter):\n",
    "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en.rstrip(\"n\"))],dtype = torch.long)\n",
    "        hi_tensor_ = torch.tensor([hi_vocab[token] for token in hi_tokenizer(raw_hi.rstrip(\"n\"))],dtype = torch.long)\n",
    "        data.append((en_tensor_, hi_tensor_))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f99df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_process(source_train_file,target_train_file)\n",
    "val_data = data_process(source_valid_file, target_valid_file)\n",
    "test_data = data_process(source_test_file, target_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "873e6467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "4\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "827e2b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "PAD_IDX = en_vocab['<pad>']\n",
    "BOS_IDX = en_vocab['<bos>']\n",
    "EOS_IDX = en_vocab['<eos>']\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f29658a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    en_batch, hi_batch = [], []\n",
    "    for (en_item, hi_item) in data_batch:\n",
    "        en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim = 0))\n",
    "        hi_batch.append(torch.cat([torch.tensor([BOS_IDX]), hi_item, torch.tensor([EOS_IDX])], dim = 0))\n",
    "    en_batch = pad_sequence(en_batch, padding_value = PAD_IDX)\n",
    "    hi_batch = pad_sequence(hi_batch, padding_value = PAD_IDX)\n",
    "    return en_batch, hi_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55518564",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DataLoader(train_data, batch_size = BATCH_SIZE, shuffle = True, collate_fn = generate_batch)\n",
    "valid_iter = DataLoader(val_data, batch_size = BATCH_SIZE, shuffle = True, collate_fn = generate_batch)\n",
    "test_iter = DataLoader(test_data, batch_size = BATCH_SIZE, shuffle = True, collate_fn = generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c9695c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "(tensor([[  2,   2,   2,   2],\n",
      "        [ 52,  52,  52, 110],\n",
      "        [  0,   0,   0, 641],\n",
      "        [  0, 380,  47,   9],\n",
      "        [  0,   6,   0,   0],\n",
      "        [ 47,   0,  27,  14],\n",
      "        [414,   0,   0,   3],\n",
      "        [ 67,  58,   0,   1],\n",
      "        [ 59,  32, 168,   1],\n",
      "        [ 37,  31,   0,   1],\n",
      "        [  0,   0,   0,   1],\n",
      "        [288,  37,   9,   1],\n",
      "        [  0, 685,   0,   1],\n",
      "        [ 59, 177,   0,   1],\n",
      "        [  0, 279,  86,   1],\n",
      "        [573,  24, 220,   1],\n",
      "        [  9,  31,  27,   1],\n",
      "        [  0,   0,   0,   1],\n",
      "        [  0,   0,   0,   1],\n",
      "        [  0, 111,   0,   1],\n",
      "        [110, 656,   0,   1],\n",
      "        [  0,   0,  31,   1],\n",
      "        [  0,  14,   0,   1],\n",
      "        [415,   3,  47,   1],\n",
      "        [174,   1,   0,   1],\n",
      "        [  0,   1,  14,   1],\n",
      "        [194,   1,   3,   1],\n",
      "        [205,   1,   1,   1],\n",
      "        [  0,   1,   1,   1],\n",
      "        [ 14,   1,   1,   1],\n",
      "        [  3,   1,   1,   1]]), tensor([[  2,   2,   2,   2],\n",
      "        [ 55,  29,   0, 114],\n",
      "        [  0,   0,   0,   0],\n",
      "        [  0,   0,  33,  13],\n",
      "        [  0,   0,   0,  62],\n",
      "        [  0,  16,   0, 692],\n",
      "        [ 29,   0, 158,   3],\n",
      "        [ 59, 100,   0,   1],\n",
      "        [ 14, 260,  46,   1],\n",
      "        [  0,  26,   0,   1],\n",
      "        [  5,   0,   0,   1],\n",
      "        [  0,   0,   0,   1],\n",
      "        [ 26, 342, 181,   1],\n",
      "        [  0,   0,   0,   1],\n",
      "        [ 13,  55,   0,   1],\n",
      "        [114, 393,  33,   1],\n",
      "        [ 34,   0,   0,   1],\n",
      "        [  0,  54, 158,   1],\n",
      "        [  5, 276,   0,   1],\n",
      "        [351,   0,  46,   1],\n",
      "        [  0,   0,   0,   1],\n",
      "        [182,  62,   0,   1],\n",
      "        [328,  18,   0,   1],\n",
      "        [ 33,   3,  17,   1],\n",
      "        [  0,   1,  35,   1],\n",
      "        [ 13,   1,   3,   1],\n",
      "        [ 46,   1,   1,   1],\n",
      "        [  0,   1,   1,   1],\n",
      "        [536,   1,   1,   1],\n",
      "        [  0,   1,   1,   1],\n",
      "        [ 22,   1,   1,   1],\n",
      "        [ 16,   1,   1,   1],\n",
      "        [ 58,   1,   1,   1],\n",
      "        [151,   1,   1,   1],\n",
      "        [ 18,   1,   1,   1],\n",
      "        [  3,   1,   1,   1]]))\n"
     ]
    }
   ],
   "source": [
    "print(len(train_iter))\n",
    "for (idx, batch) in enumerate(valid_iter):\n",
    "    print(idx)\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a70221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers:int, num_decoder_layers:int, emb_size:int, src_vocab_size:int, tgt_vocab_size:int, dim_feedforward:int = 512, dropout:float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        encoder_layer = TransformerEncoderLayer(d_model = emb_size, nhead = NHEAD, dim_feedforward = dim_feedforward)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers = num_encoder_layers)\n",
    "        decoder_layer = TransformerDecoderLayer(d_model = emb_size, nhead = NHEAD, dim_feedforward = dim_feedforward)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers = num_decoder_layers)\n",
    "        \n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout = dropout)\n",
    "        \n",
    "    def forward(self, src: Tensor, tgt: Tensor, src_mask:Tensor, tgt_mask:Tensor, src_padding_mask:Tensor, tgt_padding_mask:Tensor, memory_key_padding_mask:Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
    "        memory = self.transformer_encoder(src_emb, src_mask, src_padding_mask)\n",
    "        outs = self.transformer_decoder(tgt_emb, memory, tgt_mask, None, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "    \n",
    "    def encode(self, src:Tensor, src_mask:Tensor):\n",
    "        return self.transformer_encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)\n",
    "    \n",
    "    def decode(self, tgt:Tensor, memory:Tensor, tgt_mask:Tensor):\n",
    "        return self.transformer_decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a772d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size:int, dropout, maxlen:int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(-torch.arange(0,emb_size, 2) * math.log(10000)/emb_size)\n",
    "        pos = torch.arange(0,maxlen).reshape(maxlen,1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:,0::2] = torch.sin(pos*den)\n",
    "        pos_embedding[:,1::2] = torch.cos(pos*den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding',pos_embedding)\n",
    "    \n",
    "    def forward(self, token_embedding:Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0),:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a40eba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self,vocab_size:int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "    def forward(self, tokens:Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d97762ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz,sz), device = DEVICE)) == 1).transpose(0,1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36f407db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(src,tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "    \n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len), device = DEVICE).type(torch.bool)\n",
    "    \n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0,1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0,1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40707b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_VOCAB_SIZE = len(en_vocab)\n",
    "TGT_VOCAB_SIZE = len(hi_vocab)\n",
    "EMB_SIZE = 512\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 32\n",
    "BATCH_SIZE = 1\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "NUM_EPOCHS = 500\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1199d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim()>1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr = 0.0001, betas = (0.9,0.98), eps = 1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8ed6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_iter, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for idx, (src,tgt) in enumerate(train_iter):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        tgt_input = tgt[:-1,:]\n",
    "        \n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        \n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        tgt_out = tgt[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1,logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "    torch.save(model,'EN-HI Translator.pth')\n",
    "    return losses/len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da201cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,val_iter):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "    for idx, (src, tgt) in (enumerate(valid_iter)):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "        tgt_input = tgt[:-1,:]\n",
    "        \n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        \n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        tgt_out = tgt[1:,:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "    return losses/ len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea644376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Train loss: 6.990, Val Loss: 6.787,Epoch Time= 2.597s\n",
      "Epoch : 2, Train loss: 6.408, Val Loss: 6.807,Epoch Time= 0.799s\n",
      "Epoch : 3, Train loss: 6.183, Val Loss: 6.793,Epoch Time= 0.768s\n",
      "Epoch : 4, Train loss: 6.075, Val Loss: 6.699,Epoch Time= 0.763s\n",
      "Epoch : 5, Train loss: 5.985, Val Loss: 6.598,Epoch Time= 0.758s\n",
      "Epoch : 6, Train loss: 5.928, Val Loss: 6.547,Epoch Time= 0.763s\n",
      "Epoch : 7, Train loss: 5.891, Val Loss: 6.523,Epoch Time= 0.760s\n",
      "Epoch : 8, Train loss: 5.859, Val Loss: 6.486,Epoch Time= 0.762s\n",
      "Epoch : 9, Train loss: 5.820, Val Loss: 6.433,Epoch Time= 0.761s\n",
      "Epoch : 10, Train loss: 5.780, Val Loss: 6.384,Epoch Time= 0.762s\n",
      "Epoch : 11, Train loss: 5.756, Val Loss: 6.341,Epoch Time= 0.762s\n",
      "Epoch : 12, Train loss: 5.709, Val Loss: 6.308,Epoch Time= 0.760s\n",
      "Epoch : 13, Train loss: 5.655, Val Loss: 6.282,Epoch Time= 0.762s\n",
      "Epoch : 14, Train loss: 5.619, Val Loss: 6.258,Epoch Time= 0.762s\n",
      "Epoch : 15, Train loss: 5.566, Val Loss: 6.249,Epoch Time= 0.762s\n",
      "Epoch : 16, Train loss: 5.503, Val Loss: 6.291,Epoch Time= 0.759s\n",
      "Epoch : 17, Train loss: 5.441, Val Loss: 6.403,Epoch Time= 0.762s\n",
      "Epoch : 18, Train loss: 5.355, Val Loss: 6.493,Epoch Time= 0.767s\n",
      "Epoch : 19, Train loss: 5.281, Val Loss: 6.525,Epoch Time= 0.761s\n",
      "Epoch : 20, Train loss: 5.200, Val Loss: 6.511,Epoch Time= 0.762s\n",
      "Epoch : 21, Train loss: 5.114, Val Loss: 6.466,Epoch Time= 0.759s\n",
      "Epoch : 22, Train loss: 5.057, Val Loss: 6.421,Epoch Time= 0.761s\n",
      "Epoch : 23, Train loss: 4.957, Val Loss: 6.406,Epoch Time= 0.763s\n",
      "Epoch : 24, Train loss: 4.889, Val Loss: 6.369,Epoch Time= 0.760s\n",
      "Epoch : 25, Train loss: 4.823, Val Loss: 6.330,Epoch Time= 0.764s\n",
      "Epoch : 26, Train loss: 4.757, Val Loss: 6.326,Epoch Time= 0.797s\n",
      "Epoch : 27, Train loss: 4.671, Val Loss: 6.367,Epoch Time= 0.764s\n",
      "Epoch : 28, Train loss: 4.621, Val Loss: 6.401,Epoch Time= 0.763s\n",
      "Epoch : 29, Train loss: 4.554, Val Loss: 6.413,Epoch Time= 0.762s\n",
      "Epoch : 30, Train loss: 4.508, Val Loss: 6.385,Epoch Time= 0.761s\n",
      "Epoch : 31, Train loss: 4.436, Val Loss: 6.374,Epoch Time= 0.763s\n",
      "Epoch : 32, Train loss: 4.366, Val Loss: 6.391,Epoch Time= 0.765s\n",
      "Epoch : 33, Train loss: 4.318, Val Loss: 6.402,Epoch Time= 0.803s\n",
      "Epoch : 34, Train loss: 4.250, Val Loss: 6.400,Epoch Time= 0.810s\n",
      "Epoch : 35, Train loss: 4.184, Val Loss: 6.417,Epoch Time= 0.763s\n",
      "Epoch : 36, Train loss: 4.141, Val Loss: 6.432,Epoch Time= 0.761s\n",
      "Epoch : 37, Train loss: 4.102, Val Loss: 6.433,Epoch Time= 0.763s\n",
      "Epoch : 38, Train loss: 4.054, Val Loss: 6.404,Epoch Time= 0.763s\n",
      "Epoch : 39, Train loss: 3.996, Val Loss: 6.431,Epoch Time= 0.765s\n",
      "Epoch : 40, Train loss: 3.936, Val Loss: 6.467,Epoch Time= 0.764s\n",
      "Epoch : 41, Train loss: 3.900, Val Loss: 6.505,Epoch Time= 0.766s\n",
      "Epoch : 42, Train loss: 3.843, Val Loss: 6.512,Epoch Time= 0.765s\n",
      "Epoch : 43, Train loss: 3.810, Val Loss: 6.449,Epoch Time= 0.765s\n",
      "Epoch : 44, Train loss: 3.755, Val Loss: 6.413,Epoch Time= 0.764s\n",
      "Epoch : 45, Train loss: 3.711, Val Loss: 6.410,Epoch Time= 0.764s\n",
      "Epoch : 46, Train loss: 3.674, Val Loss: 6.406,Epoch Time= 0.766s\n",
      "Epoch : 47, Train loss: 3.630, Val Loss: 6.403,Epoch Time= 0.767s\n",
      "Epoch : 48, Train loss: 3.598, Val Loss: 6.467,Epoch Time= 0.763s\n",
      "Epoch : 49, Train loss: 3.555, Val Loss: 6.450,Epoch Time= 0.764s\n",
      "Epoch : 50, Train loss: 3.507, Val Loss: 6.455,Epoch Time= 0.769s\n",
      "Epoch : 51, Train loss: 3.488, Val Loss: 6.527,Epoch Time= 0.764s\n",
      "Epoch : 52, Train loss: 3.440, Val Loss: 6.513,Epoch Time= 0.766s\n",
      "Epoch : 53, Train loss: 3.387, Val Loss: 6.480,Epoch Time= 0.764s\n",
      "Epoch : 54, Train loss: 3.383, Val Loss: 6.549,Epoch Time= 0.766s\n",
      "Epoch : 55, Train loss: 3.345, Val Loss: 6.523,Epoch Time= 0.766s\n",
      "Epoch : 56, Train loss: 3.290, Val Loss: 6.473,Epoch Time= 0.762s\n",
      "Epoch : 57, Train loss: 3.262, Val Loss: 6.472,Epoch Time= 0.765s\n",
      "Epoch : 58, Train loss: 3.207, Val Loss: 6.506,Epoch Time= 0.765s\n",
      "Epoch : 59, Train loss: 3.187, Val Loss: 6.472,Epoch Time= 0.763s\n",
      "Epoch : 60, Train loss: 3.123, Val Loss: 6.466,Epoch Time= 0.765s\n",
      "Epoch : 61, Train loss: 3.103, Val Loss: 6.476,Epoch Time= 0.765s\n",
      "Epoch : 62, Train loss: 3.078, Val Loss: 6.428,Epoch Time= 0.766s\n",
      "Epoch : 63, Train loss: 3.034, Val Loss: 6.410,Epoch Time= 0.767s\n",
      "Epoch : 64, Train loss: 2.992, Val Loss: 6.435,Epoch Time= 0.766s\n",
      "Epoch : 65, Train loss: 2.959, Val Loss: 6.467,Epoch Time= 0.770s\n",
      "Epoch : 66, Train loss: 2.920, Val Loss: 6.450,Epoch Time= 0.766s\n",
      "Epoch : 67, Train loss: 2.885, Val Loss: 6.431,Epoch Time= 0.767s\n",
      "Epoch : 68, Train loss: 2.856, Val Loss: 6.429,Epoch Time= 0.765s\n",
      "Epoch : 69, Train loss: 2.798, Val Loss: 6.453,Epoch Time= 0.765s\n",
      "Epoch : 70, Train loss: 2.790, Val Loss: 6.484,Epoch Time= 0.768s\n",
      "Epoch : 71, Train loss: 2.742, Val Loss: 6.470,Epoch Time= 0.764s\n",
      "Epoch : 72, Train loss: 2.714, Val Loss: 6.476,Epoch Time= 0.767s\n",
      "Epoch : 73, Train loss: 2.676, Val Loss: 6.564,Epoch Time= 0.771s\n",
      "Epoch : 74, Train loss: 2.656, Val Loss: 6.507,Epoch Time= 0.764s\n",
      "Epoch : 75, Train loss: 2.587, Val Loss: 6.432,Epoch Time= 0.767s\n",
      "Epoch : 76, Train loss: 2.576, Val Loss: 6.434,Epoch Time= 0.765s\n",
      "Epoch : 77, Train loss: 2.528, Val Loss: 6.472,Epoch Time= 0.764s\n",
      "Epoch : 78, Train loss: 2.494, Val Loss: 6.505,Epoch Time= 0.770s\n",
      "Epoch : 79, Train loss: 2.441, Val Loss: 6.461,Epoch Time= 0.898s\n",
      "Epoch : 80, Train loss: 2.404, Val Loss: 6.453,Epoch Time= 0.763s\n",
      "Epoch : 81, Train loss: 2.365, Val Loss: 6.438,Epoch Time= 0.767s\n",
      "Epoch : 82, Train loss: 2.349, Val Loss: 6.499,Epoch Time= 0.765s\n",
      "Epoch : 83, Train loss: 2.324, Val Loss: 6.494,Epoch Time= 0.806s\n",
      "Epoch : 84, Train loss: 2.260, Val Loss: 6.450,Epoch Time= 0.767s\n",
      "Epoch : 85, Train loss: 2.228, Val Loss: 6.434,Epoch Time= 0.764s\n",
      "Epoch : 86, Train loss: 2.177, Val Loss: 6.497,Epoch Time= 0.764s\n",
      "Epoch : 87, Train loss: 2.192, Val Loss: 6.558,Epoch Time= 0.769s\n",
      "Epoch : 88, Train loss: 2.120, Val Loss: 6.561,Epoch Time= 0.765s\n",
      "Epoch : 89, Train loss: 2.084, Val Loss: 6.500,Epoch Time= 0.772s\n",
      "Epoch : 90, Train loss: 2.061, Val Loss: 6.456,Epoch Time= 0.768s\n",
      "Epoch : 91, Train loss: 2.021, Val Loss: 6.493,Epoch Time= 0.764s\n",
      "Epoch : 92, Train loss: 1.974, Val Loss: 6.514,Epoch Time= 0.766s\n",
      "Epoch : 93, Train loss: 1.941, Val Loss: 6.466,Epoch Time= 0.764s\n",
      "Epoch : 94, Train loss: 1.896, Val Loss: 6.444,Epoch Time= 0.766s\n",
      "Epoch : 95, Train loss: 1.849, Val Loss: 6.458,Epoch Time= 0.766s\n",
      "Epoch : 96, Train loss: 1.833, Val Loss: 6.455,Epoch Time= 0.767s\n",
      "Epoch : 97, Train loss: 1.820, Val Loss: 6.525,Epoch Time= 0.766s\n",
      "Epoch : 98, Train loss: 1.779, Val Loss: 6.563,Epoch Time= 0.765s\n",
      "Epoch : 99, Train loss: 1.756, Val Loss: 6.541,Epoch Time= 0.768s\n",
      "Epoch : 100, Train loss: 1.699, Val Loss: 6.515,Epoch Time= 0.767s\n",
      "Epoch : 101, Train loss: 1.672, Val Loss: 6.492,Epoch Time= 0.769s\n",
      "Epoch : 102, Train loss: 1.638, Val Loss: 6.478,Epoch Time= 0.769s\n",
      "Epoch : 103, Train loss: 1.620, Val Loss: 6.496,Epoch Time= 0.766s\n",
      "Epoch : 104, Train loss: 1.586, Val Loss: 6.458,Epoch Time= 0.765s\n",
      "Epoch : 105, Train loss: 1.539, Val Loss: 6.359,Epoch Time= 0.764s\n",
      "Epoch : 106, Train loss: 1.514, Val Loss: 6.388,Epoch Time= 0.767s\n",
      "Epoch : 107, Train loss: 1.495, Val Loss: 6.412,Epoch Time= 0.770s\n",
      "Epoch : 108, Train loss: 1.443, Val Loss: 6.420,Epoch Time= 0.770s\n",
      "Epoch : 109, Train loss: 1.392, Val Loss: 6.451,Epoch Time= 0.765s\n",
      "Epoch : 110, Train loss: 1.396, Val Loss: 6.491,Epoch Time= 0.771s\n",
      "Epoch : 111, Train loss: 1.344, Val Loss: 6.445,Epoch Time= 0.787s\n",
      "Epoch : 112, Train loss: 1.341, Val Loss: 6.349,Epoch Time= 0.779s\n",
      "Epoch : 113, Train loss: 1.294, Val Loss: 6.326,Epoch Time= 0.768s\n",
      "Epoch : 114, Train loss: 1.279, Val Loss: 6.396,Epoch Time= 0.805s\n",
      "Epoch : 115, Train loss: 1.243, Val Loss: 6.466,Epoch Time= 0.800s\n",
      "Epoch : 116, Train loss: 1.206, Val Loss: 6.404,Epoch Time= 0.786s\n",
      "Epoch : 117, Train loss: 1.192, Val Loss: 6.348,Epoch Time= 0.808s\n",
      "Epoch : 118, Train loss: 1.177, Val Loss: 6.401,Epoch Time= 0.768s\n",
      "Epoch : 119, Train loss: 1.139, Val Loss: 6.430,Epoch Time= 0.767s\n",
      "Epoch : 120, Train loss: 1.111, Val Loss: 6.409,Epoch Time= 0.805s\n",
      "Epoch : 121, Train loss: 1.082, Val Loss: 6.354,Epoch Time= 0.768s\n",
      "Epoch : 122, Train loss: 1.044, Val Loss: 6.352,Epoch Time= 0.769s\n",
      "Epoch : 123, Train loss: 1.038, Val Loss: 6.400,Epoch Time= 0.768s\n",
      "Epoch : 124, Train loss: 1.011, Val Loss: 6.463,Epoch Time= 0.771s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 125, Train loss: 0.989, Val Loss: 6.488,Epoch Time= 0.768s\n",
      "Epoch : 126, Train loss: 0.978, Val Loss: 6.406,Epoch Time= 0.772s\n",
      "Epoch : 127, Train loss: 0.935, Val Loss: 6.348,Epoch Time= 0.769s\n",
      "Epoch : 128, Train loss: 0.912, Val Loss: 6.386,Epoch Time= 0.766s\n",
      "Epoch : 129, Train loss: 0.899, Val Loss: 6.415,Epoch Time= 0.770s\n",
      "Epoch : 130, Train loss: 0.887, Val Loss: 6.424,Epoch Time= 0.768s\n",
      "Epoch : 131, Train loss: 0.854, Val Loss: 6.437,Epoch Time= 0.768s\n",
      "Epoch : 132, Train loss: 0.834, Val Loss: 6.491,Epoch Time= 0.769s\n",
      "Epoch : 133, Train loss: 0.826, Val Loss: 6.422,Epoch Time= 0.768s\n",
      "Epoch : 134, Train loss: 0.792, Val Loss: 6.400,Epoch Time= 0.768s\n",
      "Epoch : 135, Train loss: 0.781, Val Loss: 6.464,Epoch Time= 0.769s\n",
      "Epoch : 136, Train loss: 0.771, Val Loss: 6.519,Epoch Time= 0.768s\n",
      "Epoch : 137, Train loss: 0.736, Val Loss: 6.486,Epoch Time= 0.771s\n",
      "Epoch : 138, Train loss: 0.719, Val Loss: 6.407,Epoch Time= 0.768s\n",
      "Epoch : 139, Train loss: 0.710, Val Loss: 6.359,Epoch Time= 0.772s\n",
      "Epoch : 140, Train loss: 0.707, Val Loss: 6.335,Epoch Time= 0.767s\n",
      "Epoch : 141, Train loss: 0.685, Val Loss: 6.317,Epoch Time= 0.769s\n",
      "Epoch : 142, Train loss: 0.659, Val Loss: 6.311,Epoch Time= 0.768s\n",
      "Epoch : 143, Train loss: 0.643, Val Loss: 6.364,Epoch Time= 0.770s\n",
      "Epoch : 144, Train loss: 0.637, Val Loss: 6.399,Epoch Time= 0.769s\n",
      "Epoch : 145, Train loss: 0.622, Val Loss: 6.363,Epoch Time= 0.805s\n",
      "Epoch : 146, Train loss: 0.613, Val Loss: 6.334,Epoch Time= 0.769s\n",
      "Epoch : 147, Train loss: 0.591, Val Loss: 6.360,Epoch Time= 0.768s\n",
      "Epoch : 148, Train loss: 0.572, Val Loss: 6.404,Epoch Time= 0.768s\n",
      "Epoch : 149, Train loss: 0.571, Val Loss: 6.415,Epoch Time= 0.771s\n",
      "Epoch : 150, Train loss: 0.549, Val Loss: 6.411,Epoch Time= 0.767s\n",
      "Epoch : 151, Train loss: 0.526, Val Loss: 6.398,Epoch Time= 0.769s\n",
      "Epoch : 152, Train loss: 0.520, Val Loss: 6.373,Epoch Time= 0.770s\n",
      "Epoch : 153, Train loss: 0.518, Val Loss: 6.357,Epoch Time= 0.768s\n",
      "Epoch : 154, Train loss: 0.494, Val Loss: 6.345,Epoch Time= 0.767s\n",
      "Epoch : 155, Train loss: 0.480, Val Loss: 6.357,Epoch Time= 0.768s\n",
      "Epoch : 156, Train loss: 0.469, Val Loss: 6.364,Epoch Time= 0.768s\n",
      "Epoch : 157, Train loss: 0.468, Val Loss: 6.397,Epoch Time= 0.769s\n",
      "Epoch : 158, Train loss: 0.450, Val Loss: 6.402,Epoch Time= 0.766s\n",
      "Epoch : 159, Train loss: 0.451, Val Loss: 6.384,Epoch Time= 0.767s\n",
      "Epoch : 160, Train loss: 0.442, Val Loss: 6.404,Epoch Time= 0.770s\n",
      "Epoch : 161, Train loss: 0.416, Val Loss: 6.456,Epoch Time= 0.768s\n",
      "Epoch : 162, Train loss: 0.414, Val Loss: 6.461,Epoch Time= 0.768s\n",
      "Epoch : 163, Train loss: 0.402, Val Loss: 6.423,Epoch Time= 0.771s\n",
      "Epoch : 164, Train loss: 0.388, Val Loss: 6.413,Epoch Time= 0.767s\n",
      "Epoch : 165, Train loss: 0.386, Val Loss: 6.414,Epoch Time= 0.768s\n",
      "Epoch : 166, Train loss: 0.384, Val Loss: 6.400,Epoch Time= 0.768s\n",
      "Epoch : 167, Train loss: 0.384, Val Loss: 6.383,Epoch Time= 0.766s\n",
      "Epoch : 168, Train loss: 0.361, Val Loss: 6.395,Epoch Time= 0.769s\n",
      "Epoch : 169, Train loss: 0.346, Val Loss: 6.427,Epoch Time= 0.768s\n",
      "Epoch : 170, Train loss: 0.347, Val Loss: 6.503,Epoch Time= 0.767s\n",
      "Epoch : 171, Train loss: 0.335, Val Loss: 6.566,Epoch Time= 0.769s\n",
      "Epoch : 172, Train loss: 0.339, Val Loss: 6.515,Epoch Time= 0.765s\n",
      "Epoch : 173, Train loss: 0.323, Val Loss: 6.408,Epoch Time= 0.768s\n",
      "Epoch : 174, Train loss: 0.322, Val Loss: 6.364,Epoch Time= 0.767s\n",
      "Epoch : 175, Train loss: 0.312, Val Loss: 6.419,Epoch Time= 0.765s\n",
      "Epoch : 176, Train loss: 0.306, Val Loss: 6.500,Epoch Time= 0.765s\n",
      "Epoch : 177, Train loss: 0.304, Val Loss: 6.498,Epoch Time= 0.771s\n",
      "Epoch : 178, Train loss: 0.291, Val Loss: 6.434,Epoch Time= 0.766s\n",
      "Epoch : 179, Train loss: 0.288, Val Loss: 6.376,Epoch Time= 0.768s\n",
      "Epoch : 180, Train loss: 0.287, Val Loss: 6.373,Epoch Time= 0.768s\n",
      "Epoch : 181, Train loss: 0.273, Val Loss: 6.391,Epoch Time= 0.767s\n",
      "Epoch : 182, Train loss: 0.270, Val Loss: 6.367,Epoch Time= 0.764s\n",
      "Epoch : 183, Train loss: 0.265, Val Loss: 6.319,Epoch Time= 0.770s\n",
      "Epoch : 184, Train loss: 0.258, Val Loss: 6.341,Epoch Time= 0.766s\n",
      "Epoch : 185, Train loss: 0.260, Val Loss: 6.394,Epoch Time= 0.770s\n",
      "Epoch : 186, Train loss: 0.246, Val Loss: 6.454,Epoch Time= 0.770s\n",
      "Epoch : 187, Train loss: 0.244, Val Loss: 6.451,Epoch Time= 0.764s\n",
      "Epoch : 188, Train loss: 0.238, Val Loss: 6.394,Epoch Time= 0.773s\n",
      "Epoch : 189, Train loss: 0.238, Val Loss: 6.347,Epoch Time= 0.773s\n",
      "Epoch : 190, Train loss: 0.224, Val Loss: 6.359,Epoch Time= 0.769s\n",
      "Epoch : 191, Train loss: 0.221, Val Loss: 6.390,Epoch Time= 0.769s\n",
      "Epoch : 192, Train loss: 0.216, Val Loss: 6.409,Epoch Time= 0.768s\n",
      "Epoch : 193, Train loss: 0.216, Val Loss: 6.395,Epoch Time= 0.766s\n",
      "Epoch : 194, Train loss: 0.214, Val Loss: 6.373,Epoch Time= 0.768s\n",
      "Epoch : 195, Train loss: 0.211, Val Loss: 6.362,Epoch Time= 0.765s\n",
      "Epoch : 196, Train loss: 0.205, Val Loss: 6.364,Epoch Time= 0.767s\n",
      "Epoch : 197, Train loss: 0.199, Val Loss: 6.357,Epoch Time= 0.768s\n",
      "Epoch : 198, Train loss: 0.194, Val Loss: 6.398,Epoch Time= 0.764s\n",
      "Epoch : 199, Train loss: 0.193, Val Loss: 6.441,Epoch Time= 0.769s\n",
      "Epoch : 200, Train loss: 0.191, Val Loss: 6.455,Epoch Time= 0.768s\n",
      "Epoch : 201, Train loss: 0.186, Val Loss: 6.415,Epoch Time= 0.768s\n",
      "Epoch : 202, Train loss: 0.183, Val Loss: 6.386,Epoch Time= 0.770s\n",
      "Epoch : 203, Train loss: 0.177, Val Loss: 6.413,Epoch Time= 0.765s\n",
      "Epoch : 204, Train loss: 0.172, Val Loss: 6.470,Epoch Time= 0.768s\n",
      "Epoch : 205, Train loss: 0.172, Val Loss: 6.488,Epoch Time= 0.766s\n",
      "Epoch : 206, Train loss: 0.166, Val Loss: 6.454,Epoch Time= 0.768s\n",
      "Epoch : 207, Train loss: 0.163, Val Loss: 6.393,Epoch Time= 0.768s\n",
      "Epoch : 208, Train loss: 0.164, Val Loss: 6.346,Epoch Time= 0.768s\n",
      "Epoch : 209, Train loss: 0.163, Val Loss: 6.330,Epoch Time= 0.766s\n",
      "Epoch : 210, Train loss: 0.150, Val Loss: 6.352,Epoch Time= 0.767s\n",
      "Epoch : 211, Train loss: 0.149, Val Loss: 6.394,Epoch Time= 0.767s\n",
      "Epoch : 212, Train loss: 0.151, Val Loss: 6.415,Epoch Time= 0.771s\n",
      "Epoch : 213, Train loss: 0.146, Val Loss: 6.408,Epoch Time= 0.765s\n",
      "Epoch : 214, Train loss: 0.145, Val Loss: 6.395,Epoch Time= 0.767s\n",
      "Epoch : 215, Train loss: 0.148, Val Loss: 6.397,Epoch Time= 0.768s\n",
      "Epoch : 216, Train loss: 0.141, Val Loss: 6.415,Epoch Time= 0.769s\n",
      "Epoch : 217, Train loss: 0.137, Val Loss: 6.416,Epoch Time= 0.767s\n",
      "Epoch : 218, Train loss: 0.137, Val Loss: 6.397,Epoch Time= 0.766s\n",
      "Epoch : 219, Train loss: 0.137, Val Loss: 6.381,Epoch Time= 0.769s\n",
      "Epoch : 220, Train loss: 0.130, Val Loss: 6.386,Epoch Time= 0.766s\n",
      "Epoch : 221, Train loss: 0.135, Val Loss: 6.406,Epoch Time= 0.768s\n",
      "Epoch : 222, Train loss: 0.125, Val Loss: 6.420,Epoch Time= 0.768s\n",
      "Epoch : 223, Train loss: 0.125, Val Loss: 6.427,Epoch Time= 0.768s\n",
      "Epoch : 224, Train loss: 0.122, Val Loss: 6.422,Epoch Time= 0.770s\n",
      "Epoch : 225, Train loss: 0.124, Val Loss: 6.401,Epoch Time= 0.767s\n",
      "Epoch : 226, Train loss: 0.119, Val Loss: 6.377,Epoch Time= 0.766s\n",
      "Epoch : 227, Train loss: 0.116, Val Loss: 6.358,Epoch Time= 0.825s\n",
      "Epoch : 228, Train loss: 0.118, Val Loss: 6.344,Epoch Time= 0.775s\n",
      "Epoch : 229, Train loss: 0.114, Val Loss: 6.349,Epoch Time= 0.804s\n",
      "Epoch : 230, Train loss: 0.112, Val Loss: 6.353,Epoch Time= 0.771s\n",
      "Epoch : 231, Train loss: 0.111, Val Loss: 6.385,Epoch Time= 0.772s\n",
      "Epoch : 232, Train loss: 0.107, Val Loss: 6.423,Epoch Time= 0.763s\n",
      "Epoch : 233, Train loss: 0.111, Val Loss: 6.453,Epoch Time= 0.769s\n",
      "Epoch : 234, Train loss: 0.109, Val Loss: 6.420,Epoch Time= 0.770s\n",
      "Epoch : 235, Train loss: 0.105, Val Loss: 6.394,Epoch Time= 0.765s\n",
      "Epoch : 236, Train loss: 0.100, Val Loss: 6.375,Epoch Time= 0.767s\n",
      "Epoch : 237, Train loss: 0.101, Val Loss: 6.371,Epoch Time= 0.765s\n",
      "Epoch : 238, Train loss: 0.096, Val Loss: 6.391,Epoch Time= 0.766s\n",
      "Epoch : 239, Train loss: 0.099, Val Loss: 6.427,Epoch Time= 0.768s\n",
      "Epoch : 240, Train loss: 0.096, Val Loss: 6.460,Epoch Time= 0.766s\n",
      "Epoch : 241, Train loss: 0.093, Val Loss: 6.469,Epoch Time= 0.768s\n",
      "Epoch : 242, Train loss: 0.094, Val Loss: 6.469,Epoch Time= 0.771s\n",
      "Epoch : 243, Train loss: 0.092, Val Loss: 6.447,Epoch Time= 0.775s\n",
      "Epoch : 244, Train loss: 0.090, Val Loss: 6.434,Epoch Time= 0.766s\n",
      "Epoch : 245, Train loss: 0.089, Val Loss: 6.425,Epoch Time= 0.768s\n",
      "Epoch : 246, Train loss: 0.085, Val Loss: 6.430,Epoch Time= 0.766s\n",
      "Epoch : 247, Train loss: 0.089, Val Loss: 6.426,Epoch Time= 0.767s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 248, Train loss: 0.084, Val Loss: 6.411,Epoch Time= 0.767s\n",
      "Epoch : 249, Train loss: 0.084, Val Loss: 6.402,Epoch Time= 0.768s\n",
      "Epoch : 250, Train loss: 0.082, Val Loss: 6.401,Epoch Time= 0.767s\n",
      "Epoch : 251, Train loss: 0.083, Val Loss: 6.413,Epoch Time= 0.809s\n",
      "Epoch : 252, Train loss: 0.078, Val Loss: 6.438,Epoch Time= 0.769s\n",
      "Epoch : 253, Train loss: 0.078, Val Loss: 6.442,Epoch Time= 0.770s\n",
      "Epoch : 254, Train loss: 0.079, Val Loss: 6.413,Epoch Time= 0.773s\n",
      "Epoch : 255, Train loss: 0.073, Val Loss: 6.385,Epoch Time= 0.766s\n",
      "Epoch : 256, Train loss: 0.077, Val Loss: 6.393,Epoch Time= 0.767s\n",
      "Epoch : 257, Train loss: 0.072, Val Loss: 6.431,Epoch Time= 0.768s\n",
      "Epoch : 258, Train loss: 0.074, Val Loss: 6.464,Epoch Time= 0.766s\n",
      "Epoch : 259, Train loss: 0.075, Val Loss: 6.468,Epoch Time= 0.769s\n",
      "Epoch : 260, Train loss: 0.072, Val Loss: 6.432,Epoch Time= 0.767s\n",
      "Epoch : 261, Train loss: 0.073, Val Loss: 6.380,Epoch Time= 0.769s\n",
      "Epoch : 262, Train loss: 0.070, Val Loss: 6.365,Epoch Time= 0.767s\n",
      "Epoch : 263, Train loss: 0.066, Val Loss: 6.384,Epoch Time= 0.767s\n",
      "Epoch : 264, Train loss: 0.068, Val Loss: 6.421,Epoch Time= 0.767s\n",
      "Epoch : 265, Train loss: 0.066, Val Loss: 6.429,Epoch Time= 0.768s\n",
      "Epoch : 266, Train loss: 0.064, Val Loss: 6.421,Epoch Time= 0.768s\n",
      "Epoch : 267, Train loss: 0.063, Val Loss: 6.404,Epoch Time= 0.767s\n",
      "Epoch : 268, Train loss: 0.063, Val Loss: 6.391,Epoch Time= 0.770s\n",
      "Epoch : 269, Train loss: 0.062, Val Loss: 6.409,Epoch Time= 0.768s\n",
      "Epoch : 270, Train loss: 0.061, Val Loss: 6.433,Epoch Time= 0.767s\n",
      "Epoch : 271, Train loss: 0.061, Val Loss: 6.451,Epoch Time= 0.768s\n",
      "Epoch : 272, Train loss: 0.062, Val Loss: 6.456,Epoch Time= 0.766s\n",
      "Epoch : 273, Train loss: 0.062, Val Loss: 6.436,Epoch Time= 0.768s\n",
      "Epoch : 274, Train loss: 0.061, Val Loss: 6.417,Epoch Time= 0.768s\n",
      "Epoch : 275, Train loss: 0.059, Val Loss: 6.404,Epoch Time= 0.769s\n",
      "Epoch : 276, Train loss: 0.060, Val Loss: 6.416,Epoch Time= 0.767s\n",
      "Epoch : 277, Train loss: 0.056, Val Loss: 6.435,Epoch Time= 0.766s\n",
      "Epoch : 278, Train loss: 0.057, Val Loss: 6.438,Epoch Time= 0.766s\n",
      "Epoch : 279, Train loss: 0.054, Val Loss: 6.418,Epoch Time= 0.767s\n",
      "Epoch : 280, Train loss: 0.054, Val Loss: 6.387,Epoch Time= 0.766s\n",
      "Epoch : 281, Train loss: 0.054, Val Loss: 6.369,Epoch Time= 0.769s\n",
      "Epoch : 282, Train loss: 0.054, Val Loss: 6.379,Epoch Time= 0.768s\n",
      "Epoch : 283, Train loss: 0.051, Val Loss: 6.415,Epoch Time= 0.801s\n",
      "Epoch : 284, Train loss: 0.051, Val Loss: 6.447,Epoch Time= 0.769s\n",
      "Epoch : 285, Train loss: 0.049, Val Loss: 6.473,Epoch Time= 0.769s\n",
      "Epoch : 286, Train loss: 0.049, Val Loss: 6.489,Epoch Time= 0.767s\n",
      "Epoch : 287, Train loss: 0.050, Val Loss: 6.464,Epoch Time= 0.767s\n",
      "Epoch : 288, Train loss: 0.049, Val Loss: 6.429,Epoch Time= 0.769s\n",
      "Epoch : 289, Train loss: 0.049, Val Loss: 6.427,Epoch Time= 0.767s\n",
      "Epoch : 290, Train loss: 0.047, Val Loss: 6.446,Epoch Time= 0.770s\n",
      "Epoch : 291, Train loss: 0.045, Val Loss: 6.478,Epoch Time= 0.767s\n",
      "Epoch : 292, Train loss: 0.048, Val Loss: 6.486,Epoch Time= 0.768s\n",
      "Epoch : 293, Train loss: 0.045, Val Loss: 6.459,Epoch Time= 0.767s\n",
      "Epoch : 294, Train loss: 0.044, Val Loss: 6.415,Epoch Time= 0.767s\n",
      "Epoch : 295, Train loss: 0.044, Val Loss: 6.390,Epoch Time= 0.768s\n",
      "Epoch : 296, Train loss: 0.043, Val Loss: 6.399,Epoch Time= 0.768s\n",
      "Epoch : 297, Train loss: 0.043, Val Loss: 6.446,Epoch Time= 0.767s\n",
      "Epoch : 298, Train loss: 0.043, Val Loss: 6.471,Epoch Time= 0.767s\n",
      "Epoch : 299, Train loss: 0.040, Val Loss: 6.486,Epoch Time= 0.769s\n",
      "Epoch : 300, Train loss: 0.042, Val Loss: 6.481,Epoch Time= 0.769s\n",
      "Epoch : 301, Train loss: 0.041, Val Loss: 6.456,Epoch Time= 0.773s\n",
      "Epoch : 302, Train loss: 0.041, Val Loss: 6.434,Epoch Time= 0.767s\n",
      "Epoch : 303, Train loss: 0.039, Val Loss: 6.413,Epoch Time= 0.775s\n",
      "Epoch : 304, Train loss: 0.040, Val Loss: 6.420,Epoch Time= 0.772s\n",
      "Epoch : 305, Train loss: 0.038, Val Loss: 6.453,Epoch Time= 0.770s\n",
      "Epoch : 306, Train loss: 0.039, Val Loss: 6.475,Epoch Time= 0.770s\n",
      "Epoch : 307, Train loss: 0.037, Val Loss: 6.474,Epoch Time= 0.770s\n",
      "Epoch : 308, Train loss: 0.039, Val Loss: 6.456,Epoch Time= 0.768s\n",
      "Epoch : 309, Train loss: 0.039, Val Loss: 6.427,Epoch Time= 0.766s\n",
      "Epoch : 310, Train loss: 0.036, Val Loss: 6.410,Epoch Time= 0.769s\n",
      "Epoch : 311, Train loss: 0.037, Val Loss: 6.420,Epoch Time= 0.766s\n",
      "Epoch : 312, Train loss: 0.036, Val Loss: 6.426,Epoch Time= 0.766s\n",
      "Epoch : 313, Train loss: 0.038, Val Loss: 6.452,Epoch Time= 0.769s\n",
      "Epoch : 314, Train loss: 0.034, Val Loss: 6.471,Epoch Time= 0.767s\n",
      "Epoch : 315, Train loss: 0.037, Val Loss: 6.469,Epoch Time= 0.769s\n",
      "Epoch : 316, Train loss: 0.035, Val Loss: 6.447,Epoch Time= 0.768s\n",
      "Epoch : 317, Train loss: 0.035, Val Loss: 6.413,Epoch Time= 0.767s\n",
      "Epoch : 318, Train loss: 0.032, Val Loss: 6.378,Epoch Time= 0.767s\n",
      "Epoch : 319, Train loss: 0.035, Val Loss: 6.382,Epoch Time= 0.767s\n",
      "Epoch : 320, Train loss: 0.033, Val Loss: 6.428,Epoch Time= 0.767s\n",
      "Epoch : 321, Train loss: 0.034, Val Loss: 6.479,Epoch Time= 0.769s\n",
      "Epoch : 322, Train loss: 0.031, Val Loss: 6.509,Epoch Time= 0.767s\n",
      "Epoch : 323, Train loss: 0.031, Val Loss: 6.498,Epoch Time= 0.768s\n",
      "Epoch : 324, Train loss: 0.032, Val Loss: 6.442,Epoch Time= 0.767s\n",
      "Epoch : 325, Train loss: 0.031, Val Loss: 6.404,Epoch Time= 0.767s\n",
      "Epoch : 326, Train loss: 0.030, Val Loss: 6.395,Epoch Time= 0.765s\n",
      "Epoch : 327, Train loss: 0.033, Val Loss: 6.423,Epoch Time= 0.764s\n",
      "Epoch : 328, Train loss: 0.028, Val Loss: 6.457,Epoch Time= 0.770s\n",
      "Epoch : 329, Train loss: 0.029, Val Loss: 6.493,Epoch Time= 0.767s\n",
      "Epoch : 330, Train loss: 0.030, Val Loss: 6.508,Epoch Time= 0.805s\n",
      "Epoch : 331, Train loss: 0.028, Val Loss: 6.496,Epoch Time= 0.765s\n",
      "Epoch : 332, Train loss: 0.029, Val Loss: 6.480,Epoch Time= 0.771s\n",
      "Epoch : 333, Train loss: 0.029, Val Loss: 6.459,Epoch Time= 0.820s\n",
      "Epoch : 334, Train loss: 0.027, Val Loss: 6.455,Epoch Time= 0.768s\n",
      "Epoch : 335, Train loss: 0.028, Val Loss: 6.468,Epoch Time= 0.768s\n",
      "Epoch : 336, Train loss: 0.027, Val Loss: 6.498,Epoch Time= 0.772s\n",
      "Epoch : 337, Train loss: 0.027, Val Loss: 6.527,Epoch Time= 0.768s\n",
      "Epoch : 338, Train loss: 0.026, Val Loss: 6.509,Epoch Time= 0.769s\n",
      "Epoch : 339, Train loss: 0.029, Val Loss: 6.468,Epoch Time= 0.766s\n",
      "Epoch : 340, Train loss: 0.025, Val Loss: 6.424,Epoch Time= 0.769s\n",
      "Epoch : 341, Train loss: 0.025, Val Loss: 6.411,Epoch Time= 0.767s\n",
      "Epoch : 342, Train loss: 0.025, Val Loss: 6.435,Epoch Time= 0.767s\n",
      "Epoch : 343, Train loss: 0.025, Val Loss: 6.485,Epoch Time= 0.771s\n",
      "Epoch : 344, Train loss: 0.025, Val Loss: 6.534,Epoch Time= 0.768s\n",
      "Epoch : 345, Train loss: 0.024, Val Loss: 6.567,Epoch Time= 0.767s\n",
      "Epoch : 346, Train loss: 0.025, Val Loss: 6.560,Epoch Time= 0.768s\n",
      "Epoch : 347, Train loss: 0.024, Val Loss: 6.523,Epoch Time= 0.769s\n",
      "Epoch : 348, Train loss: 0.023, Val Loss: 6.496,Epoch Time= 0.766s\n",
      "Epoch : 349, Train loss: 0.024, Val Loss: 6.473,Epoch Time= 0.767s\n",
      "Epoch : 350, Train loss: 0.022, Val Loss: 6.485,Epoch Time= 0.766s\n",
      "Epoch : 351, Train loss: 0.023, Val Loss: 6.503,Epoch Time= 0.767s\n",
      "Epoch : 352, Train loss: 0.022, Val Loss: 6.523,Epoch Time= 0.765s\n",
      "Epoch : 353, Train loss: 0.023, Val Loss: 6.538,Epoch Time= 0.766s\n",
      "Epoch : 354, Train loss: 0.022, Val Loss: 6.539,Epoch Time= 0.767s\n",
      "Epoch : 355, Train loss: 0.023, Val Loss: 6.529,Epoch Time= 0.769s\n",
      "Epoch : 356, Train loss: 0.020, Val Loss: 6.523,Epoch Time= 0.768s\n",
      "Epoch : 357, Train loss: 0.021, Val Loss: 6.521,Epoch Time= 0.769s\n",
      "Epoch : 358, Train loss: 0.020, Val Loss: 6.523,Epoch Time= 0.766s\n",
      "Epoch : 359, Train loss: 0.021, Val Loss: 6.514,Epoch Time= 0.767s\n",
      "Epoch : 360, Train loss: 0.020, Val Loss: 6.497,Epoch Time= 0.767s\n",
      "Epoch : 361, Train loss: 0.022, Val Loss: 6.479,Epoch Time= 0.766s\n",
      "Epoch : 362, Train loss: 0.021, Val Loss: 6.448,Epoch Time= 0.766s\n",
      "Epoch : 363, Train loss: 0.020, Val Loss: 6.440,Epoch Time= 0.766s\n",
      "Epoch : 364, Train loss: 0.018, Val Loss: 6.442,Epoch Time= 0.767s\n",
      "Epoch : 365, Train loss: 0.022, Val Loss: 6.460,Epoch Time= 0.768s\n",
      "Epoch : 366, Train loss: 0.019, Val Loss: 6.487,Epoch Time= 0.806s\n",
      "Epoch : 367, Train loss: 0.018, Val Loss: 6.508,Epoch Time= 0.810s\n",
      "Epoch : 368, Train loss: 0.019, Val Loss: 6.526,Epoch Time= 0.769s\n",
      "Epoch : 369, Train loss: 0.019, Val Loss: 6.517,Epoch Time= 0.779s\n",
      "Epoch : 370, Train loss: 0.022, Val Loss: 6.476,Epoch Time= 0.766s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 371, Train loss: 0.018, Val Loss: 6.451,Epoch Time= 0.767s\n",
      "Epoch : 372, Train loss: 0.018, Val Loss: 6.456,Epoch Time= 0.767s\n",
      "Epoch : 373, Train loss: 0.019, Val Loss: 6.488,Epoch Time= 0.769s\n",
      "Epoch : 374, Train loss: 0.019, Val Loss: 6.530,Epoch Time= 0.767s\n",
      "Epoch : 375, Train loss: 0.017, Val Loss: 6.582,Epoch Time= 0.770s\n",
      "Epoch : 376, Train loss: 0.017, Val Loss: 6.612,Epoch Time= 0.769s\n",
      "Epoch : 377, Train loss: 0.019, Val Loss: 6.611,Epoch Time= 0.769s\n",
      "Epoch : 378, Train loss: 0.018, Val Loss: 6.572,Epoch Time= 0.769s\n",
      "Epoch : 379, Train loss: 0.018, Val Loss: 6.530,Epoch Time= 0.768s\n",
      "Epoch : 380, Train loss: 0.017, Val Loss: 6.487,Epoch Time= 0.766s\n",
      "Epoch : 381, Train loss: 0.019, Val Loss: 6.461,Epoch Time= 0.767s\n",
      "Epoch : 382, Train loss: 0.017, Val Loss: 6.456,Epoch Time= 0.768s\n",
      "Epoch : 383, Train loss: 0.016, Val Loss: 6.467,Epoch Time= 0.768s\n",
      "Epoch : 384, Train loss: 0.017, Val Loss: 6.490,Epoch Time= 0.768s\n",
      "Epoch : 385, Train loss: 0.016, Val Loss: 6.513,Epoch Time= 0.765s\n",
      "Epoch : 386, Train loss: 0.016, Val Loss: 6.532,Epoch Time= 0.772s\n",
      "Epoch : 387, Train loss: 0.016, Val Loss: 6.549,Epoch Time= 0.767s\n",
      "Epoch : 388, Train loss: 0.015, Val Loss: 6.568,Epoch Time= 0.770s\n",
      "Epoch : 389, Train loss: 0.015, Val Loss: 6.579,Epoch Time= 0.772s\n",
      "Epoch : 390, Train loss: 0.015, Val Loss: 6.583,Epoch Time= 0.769s\n",
      "Epoch : 391, Train loss: 0.015, Val Loss: 6.577,Epoch Time= 0.768s\n",
      "Epoch : 392, Train loss: 0.015, Val Loss: 6.575,Epoch Time= 0.770s\n",
      "Epoch : 393, Train loss: 0.014, Val Loss: 6.571,Epoch Time= 0.769s\n",
      "Epoch : 394, Train loss: 0.013, Val Loss: 6.562,Epoch Time= 0.769s\n",
      "Epoch : 395, Train loss: 0.015, Val Loss: 6.555,Epoch Time= 0.769s\n",
      "Epoch : 396, Train loss: 0.014, Val Loss: 6.550,Epoch Time= 0.768s\n",
      "Epoch : 397, Train loss: 0.013, Val Loss: 6.561,Epoch Time= 0.771s\n",
      "Epoch : 398, Train loss: 0.013, Val Loss: 6.581,Epoch Time= 0.769s\n",
      "Epoch : 399, Train loss: 0.014, Val Loss: 6.598,Epoch Time= 0.769s\n",
      "Epoch : 400, Train loss: 0.014, Val Loss: 6.611,Epoch Time= 0.771s\n",
      "Epoch : 401, Train loss: 0.013, Val Loss: 6.617,Epoch Time= 0.769s\n",
      "Epoch : 402, Train loss: 0.012, Val Loss: 6.619,Epoch Time= 0.766s\n",
      "Epoch : 403, Train loss: 0.013, Val Loss: 6.608,Epoch Time= 0.768s\n",
      "Epoch : 404, Train loss: 0.013, Val Loss: 6.604,Epoch Time= 0.769s\n",
      "Epoch : 405, Train loss: 0.013, Val Loss: 6.598,Epoch Time= 0.768s\n",
      "Epoch : 406, Train loss: 0.012, Val Loss: 6.591,Epoch Time= 0.779s\n",
      "Epoch : 407, Train loss: 0.012, Val Loss: 6.582,Epoch Time= 0.769s\n",
      "Epoch : 408, Train loss: 0.013, Val Loss: 6.579,Epoch Time= 0.767s\n",
      "Epoch : 409, Train loss: 0.013, Val Loss: 6.584,Epoch Time= 0.771s\n",
      "Epoch : 410, Train loss: 0.013, Val Loss: 6.579,Epoch Time= 0.770s\n",
      "Epoch : 411, Train loss: 0.013, Val Loss: 6.560,Epoch Time= 0.769s\n",
      "Epoch : 412, Train loss: 0.012, Val Loss: 6.531,Epoch Time= 0.770s\n",
      "Epoch : 413, Train loss: 0.013, Val Loss: 6.498,Epoch Time= 0.769s\n",
      "Epoch : 414, Train loss: 0.012, Val Loss: 6.502,Epoch Time= 0.769s\n",
      "Epoch : 415, Train loss: 0.012, Val Loss: 6.571,Epoch Time= 0.772s\n",
      "Epoch : 416, Train loss: 0.012, Val Loss: 6.629,Epoch Time= 0.768s\n",
      "Epoch : 417, Train loss: 0.013, Val Loss: 6.653,Epoch Time= 0.767s\n",
      "Epoch : 418, Train loss: 0.011, Val Loss: 6.644,Epoch Time= 0.768s\n",
      "Epoch : 419, Train loss: 0.011, Val Loss: 6.619,Epoch Time= 0.768s\n",
      "Epoch : 420, Train loss: 0.011, Val Loss: 6.594,Epoch Time= 0.768s\n",
      "Epoch : 421, Train loss: 0.012, Val Loss: 6.568,Epoch Time= 0.768s\n",
      "Epoch : 422, Train loss: 0.012, Val Loss: 6.570,Epoch Time= 0.769s\n",
      "Epoch : 423, Train loss: 0.011, Val Loss: 6.596,Epoch Time= 0.768s\n",
      "Epoch : 424, Train loss: 0.010, Val Loss: 6.630,Epoch Time= 0.769s\n",
      "Epoch : 425, Train loss: 0.011, Val Loss: 6.659,Epoch Time= 0.769s\n",
      "Epoch : 426, Train loss: 0.010, Val Loss: 6.688,Epoch Time= 0.770s\n",
      "Epoch : 427, Train loss: 0.011, Val Loss: 6.707,Epoch Time= 0.770s\n",
      "Epoch : 428, Train loss: 0.010, Val Loss: 6.716,Epoch Time= 0.767s\n",
      "Epoch : 429, Train loss: 0.010, Val Loss: 6.693,Epoch Time= 0.771s\n",
      "Epoch : 430, Train loss: 0.011, Val Loss: 6.657,Epoch Time= 0.768s\n",
      "Epoch : 431, Train loss: 0.010, Val Loss: 6.603,Epoch Time= 0.768s\n",
      "Epoch : 432, Train loss: 0.010, Val Loss: 6.569,Epoch Time= 0.769s\n",
      "Epoch : 433, Train loss: 0.009, Val Loss: 6.565,Epoch Time= 0.768s\n",
      "Epoch : 434, Train loss: 0.011, Val Loss: 6.583,Epoch Time= 0.807s\n",
      "Epoch : 435, Train loss: 0.009, Val Loss: 6.628,Epoch Time= 0.816s\n",
      "Epoch : 436, Train loss: 0.010, Val Loss: 6.685,Epoch Time= 0.779s\n",
      "Epoch : 437, Train loss: 0.009, Val Loss: 6.695,Epoch Time= 0.771s\n",
      "Epoch : 438, Train loss: 0.009, Val Loss: 6.671,Epoch Time= 0.772s\n",
      "Epoch : 439, Train loss: 0.009, Val Loss: 6.645,Epoch Time= 0.769s\n",
      "Epoch : 440, Train loss: 0.010, Val Loss: 6.619,Epoch Time= 0.767s\n",
      "Epoch : 441, Train loss: 0.009, Val Loss: 6.609,Epoch Time= 0.811s\n",
      "Epoch : 442, Train loss: 0.009, Val Loss: 6.615,Epoch Time= 0.771s\n",
      "Epoch : 443, Train loss: 0.009, Val Loss: 6.629,Epoch Time= 0.766s\n",
      "Epoch : 444, Train loss: 0.008, Val Loss: 6.637,Epoch Time= 0.768s\n",
      "Epoch : 445, Train loss: 0.008, Val Loss: 6.627,Epoch Time= 0.767s\n",
      "Epoch : 446, Train loss: 0.008, Val Loss: 6.622,Epoch Time= 0.768s\n",
      "Epoch : 447, Train loss: 0.008, Val Loss: 6.624,Epoch Time= 0.769s\n",
      "Epoch : 448, Train loss: 0.008, Val Loss: 6.631,Epoch Time= 0.770s\n",
      "Epoch : 449, Train loss: 0.008, Val Loss: 6.633,Epoch Time= 0.767s\n",
      "Epoch : 450, Train loss: 0.008, Val Loss: 6.635,Epoch Time= 0.768s\n",
      "Epoch : 451, Train loss: 0.008, Val Loss: 6.663,Epoch Time= 0.768s\n",
      "Epoch : 452, Train loss: 0.009, Val Loss: 6.690,Epoch Time= 0.800s\n",
      "Epoch : 453, Train loss: 0.009, Val Loss: 6.731,Epoch Time= 0.771s\n",
      "Epoch : 454, Train loss: 0.008, Val Loss: 6.746,Epoch Time= 0.768s\n",
      "Epoch : 455, Train loss: 0.009, Val Loss: 6.706,Epoch Time= 0.769s\n",
      "Epoch : 456, Train loss: 0.008, Val Loss: 6.683,Epoch Time= 0.768s\n",
      "Epoch : 457, Train loss: 0.008, Val Loss: 6.663,Epoch Time= 0.768s\n",
      "Epoch : 458, Train loss: 0.008, Val Loss: 6.652,Epoch Time= 0.770s\n",
      "Epoch : 459, Train loss: 0.010, Val Loss: 6.659,Epoch Time= 0.770s\n",
      "Epoch : 460, Train loss: 0.008, Val Loss: 6.677,Epoch Time= 0.768s\n",
      "Epoch : 461, Train loss: 0.007, Val Loss: 6.672,Epoch Time= 0.771s\n",
      "Epoch : 462, Train loss: 0.008, Val Loss: 6.621,Epoch Time= 0.770s\n",
      "Epoch : 463, Train loss: 0.007, Val Loss: 6.594,Epoch Time= 0.769s\n",
      "Epoch : 464, Train loss: 0.007, Val Loss: 6.580,Epoch Time= 0.770s\n",
      "Epoch : 465, Train loss: 0.008, Val Loss: 6.590,Epoch Time= 0.769s\n",
      "Epoch : 466, Train loss: 0.009, Val Loss: 6.619,Epoch Time= 0.770s\n",
      "Epoch : 467, Train loss: 0.008, Val Loss: 6.674,Epoch Time= 0.768s\n",
      "Epoch : 468, Train loss: 0.008, Val Loss: 6.704,Epoch Time= 0.769s\n",
      "Epoch : 469, Train loss: 0.008, Val Loss: 6.697,Epoch Time= 0.772s\n",
      "Epoch : 470, Train loss: 0.010, Val Loss: 6.686,Epoch Time= 0.808s\n",
      "Epoch : 471, Train loss: 0.007, Val Loss: 6.657,Epoch Time= 0.813s\n",
      "Epoch : 472, Train loss: 0.008, Val Loss: 6.628,Epoch Time= 0.772s\n",
      "Epoch : 473, Train loss: 0.008, Val Loss: 6.616,Epoch Time= 0.769s\n",
      "Epoch : 474, Train loss: 0.008, Val Loss: 6.634,Epoch Time= 0.769s\n",
      "Epoch : 475, Train loss: 0.008, Val Loss: 6.632,Epoch Time= 0.769s\n",
      "Epoch : 476, Train loss: 0.007, Val Loss: 6.624,Epoch Time= 0.767s\n",
      "Epoch : 477, Train loss: 0.008, Val Loss: 6.625,Epoch Time= 0.768s\n",
      "Epoch : 478, Train loss: 0.007, Val Loss: 6.642,Epoch Time= 0.770s\n",
      "Epoch : 479, Train loss: 0.007, Val Loss: 6.668,Epoch Time= 0.769s\n",
      "Epoch : 480, Train loss: 0.007, Val Loss: 6.694,Epoch Time= 0.769s\n",
      "Epoch : 481, Train loss: 0.007, Val Loss: 6.725,Epoch Time= 0.768s\n",
      "Epoch : 482, Train loss: 0.006, Val Loss: 6.766,Epoch Time= 0.769s\n",
      "Epoch : 483, Train loss: 0.009, Val Loss: 6.791,Epoch Time= 0.769s\n",
      "Epoch : 484, Train loss: 0.006, Val Loss: 6.791,Epoch Time= 0.771s\n",
      "Epoch : 485, Train loss: 0.006, Val Loss: 6.766,Epoch Time= 0.770s\n",
      "Epoch : 486, Train loss: 0.007, Val Loss: 6.723,Epoch Time= 0.807s\n",
      "Epoch : 487, Train loss: 0.006, Val Loss: 6.692,Epoch Time= 0.770s\n",
      "Epoch : 488, Train loss: 0.008, Val Loss: 6.644,Epoch Time= 0.768s\n",
      "Epoch : 489, Train loss: 0.006, Val Loss: 6.625,Epoch Time= 0.773s\n",
      "Epoch : 490, Train loss: 0.007, Val Loss: 6.631,Epoch Time= 0.767s\n",
      "Epoch : 491, Train loss: 0.006, Val Loss: 6.659,Epoch Time= 0.769s\n",
      "Epoch : 492, Train loss: 0.006, Val Loss: 6.699,Epoch Time= 0.771s\n",
      "Epoch : 493, Train loss: 0.006, Val Loss: 6.754,Epoch Time= 0.771s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 494, Train loss: 0.006, Val Loss: 6.785,Epoch Time= 0.768s\n",
      "Epoch : 495, Train loss: 0.006, Val Loss: 6.765,Epoch Time= 0.768s\n",
      "Epoch : 496, Train loss: 0.006, Val Loss: 6.722,Epoch Time= 0.769s\n",
      "Epoch : 497, Train loss: 0.011, Val Loss: 6.678,Epoch Time= 0.794s\n",
      "Epoch : 498, Train loss: 0.007, Val Loss: 6.645,Epoch Time= 0.770s\n",
      "Epoch : 499, Train loss: 0.006, Val Loss: 6.615,Epoch Time= 0.790s\n",
      "Epoch : 500, Train loss: 0.007, Val Loss: 6.629,Epoch Time= 0.779s\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(transformer, train_iter, optimizer)\n",
    "    end_time = time.time()\n",
    "    val_loss = evaluate(transformer, valid_iter)\n",
    "    print((f\"Epoch : {epoch}, Train loss: {train_loss:.3f}, Val Loss: {val_loss:.3f},\"\n",
    "           f\"Epoch Time= {(end_time - start_time):.3f}s\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "162bdddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1,1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len -1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        memory_mask = torch.zeros(ys.shape[0], memory.shape[0]).to(DEVICE).type(torch.bool)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0)).type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0,1)\n",
    "        prob = model.generator(out[:,-1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        \n",
    "        ys = torch.cat([ys, torch.ones(1,1).type_as(src.data).fill_(next_word)],dim = 0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8115b87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = 'a'\n",
    "something = en_vocab[tok]\n",
    "something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "361d90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src, src_vocab, tgt_vocab, src_tokenizer):\n",
    "    model.eval()\n",
    "    itos = tgt_vocab.get_itos()\n",
    "    tokens = [BOS_IDX] + [src_vocab[tok] for tok in src_tokenizer(src)] + [EOS_IDX]\n",
    "    num_tokens = len(tokens)\n",
    "    src = (torch.LongTensor(tokens).reshape(num_tokens,1))\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(model, src, src_mask, max_len = num_tokens + 5, start_symbol = BOS_IDX).flatten()\n",
    "    return \" \".join([itos[tok] for tok in tgt_tokens]).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "20076ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: This changed slowly \n",
      "Hindi:  धीरे धीरे धीरे ये सब बदला \n"
     ]
    }
   ],
   "source": [
    "text = \"This changed slowly\"\n",
    "output = translate(transformer, text, en_vocab, hi_vocab, en_tokenizer)\n",
    "print(f'English: {text} \\nHindi: {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e52a192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1826e41f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
